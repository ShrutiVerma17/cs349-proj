{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1883387e-2829-499b-baeb-9e88d7da49fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# from collections.abc import Sequence\n",
    "from typing import Sequence\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "_SSM_NAME = \"JackFram/llama-160m\"\n",
    "_LLM_NAME = 'openlm-research/open_llama_3b_v2'\n",
    "device = \"cuda\"\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "tokenizer = AutoTokenizer.from_pretrained(_SSM_NAME)\n",
    "ssm = AutoModelForCausalLM.from_pretrained(_SSM_NAME).cuda()\n",
    "llm = AutoModelForCausalLM.from_pretrained(_LLM_NAME).cuda()\n",
    "N_ITERATIONS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d0fe080-cfbb-4cbb-8d2a-22f3acf2a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_token_tree(\n",
    "    expansion_config: Sequence[int],\n",
    "    prompt: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: AutoModelForCausalLM,\n",
    "):\n",
    "    \"\"\"Create token tree following Figure 3 in the paper.\n",
    "\n",
    "    We don't need \"real\" tokens for our experiments - just\n",
    "    random integers would work too - but might as well.\n",
    "\n",
    "    Figure 3 illustrates the <k1, k2, ...> expansion approach they\n",
    "    use to create token trees. We can use each of the top_k tokens from\n",
    "    a single model to create the same tree structure.\n",
    "\n",
    "    Args:\n",
    "        expansion_config: A sequence of integers representing how much to\n",
    "            branch at each generation step.\n",
    "        prompt: Initial prompt.\n",
    "        tokenizer: HF tokenizer.\n",
    "        model: HF generative model.\n",
    "    \"\"\"\n",
    "    assert expansion_config\n",
    "    current_tree = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "    for k in expansion_config:\n",
    "        output = model.generate(\n",
    "            current_tree,\n",
    "            max_new_tokens=1,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "        # Take the top_k tokens from the 1 generation step we've done\n",
    "        top_k = torch.topk(output.scores[-1], k=k, dim=-1).indices.reshape(-1, 1)\n",
    "        current_tree = torch.repeat_interleave(current_tree, k, dim=0)\n",
    "        # Join the top_k tokens to the current tree\n",
    "        current_tree = torch.cat((current_tree, top_k), dim=-1)\n",
    "\n",
    "    return current_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f853eaf-c00b-46b7-8424-7026c0bc8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tree_model_inputs(sequences):\n",
    "    # input_1 = torch.unique(torch.flatten(sequences), sorted=False)\n",
    "    flat = torch.flatten(sequences).tolist()\n",
    "    unique = []\n",
    "    for tok in flat:\n",
    "        if tok not in unique:\n",
    "            unique.append(tok)\n",
    "    # input is list of unique tokens\n",
    "    input_1 = torch.tensor([unique]).to(device)\n",
    "\n",
    "    a = input_1.shape[-1]\n",
    "    mask_1 = np.zeros((a, a))\n",
    "    positions = [-1] * len(unique)\n",
    "    \n",
    "    for seq in sequences:\n",
    "        branch_progress = []\n",
    "        for (pos, tok) in enumerate(seq):\n",
    "            input_1_idx = unique.index(tok)\n",
    "            positions[input_1_idx] = pos\n",
    "            branch_progress.append(input_1_idx)\n",
    "            for idx in branch_progress:\n",
    "                mask_1[input_1_idx][idx] = 1\n",
    "    mask_1 = torch.tensor(mask_1, device=device, dtype=torch.int64)\n",
    "    mask_1 = mask_1.unsqueeze(0).unsqueeze(0).to(device).int()\n",
    "    position_ids_1 = torch.tensor([positions], device=device, dtype=torch.int64)\n",
    "    return (input_1, mask_1, position_ids_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3de2107-a040-4a06-8b3d-8b0e268a43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_normal(input, N_iterations, model: AutoModelForCausalLM):\n",
    "    total_time = 0.0\n",
    "    for i in range(N_iterations):\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            logits = model(input_ids=input).logits\n",
    "            end = time.time()\n",
    "        total_time += end - start\n",
    "    return (total_time / N_iterations)\n",
    "\n",
    "def time_tree(input, mask, position_ids, N_iterations, model: AutoModelForCausalLM):\n",
    "    total_time = 0.0\n",
    "    for i in range(N_iterations):\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            logits = model.forward(input_ids=input, attention_mask=mask, position_ids=position_ids).logits\n",
    "            end = time.time()\n",
    "        total_time += end - start\n",
    "    return (total_time / N_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e0930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, ProfilerActivity, schedule\n",
    "\n",
    "# Guide: https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n",
    "\n",
    "_N_ITERATIONS = 10\n",
    "_WAIT_STEPS = 1\n",
    "_WARMUP_STEPS = 1\n",
    "schedule_params = {\n",
    "    'wait': _WAIT_STEPS,\n",
    "    'warmup': _WARMUP_STEPS,\n",
    "    'active': _N_ITERATIONS - _WAIT_STEPS - _WARMUP_STEPS,\n",
    "}\n",
    "profiler_kwargs = {\n",
    "    'activities': [ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    'profile_memory': True,\n",
    "    'schedule': schedule(**schedule_params),\n",
    "}\n",
    "\n",
    "def print_normal_profile_stats(input, model):\n",
    "    with torch.inference_mode(), profile(**profiler_kwargs) as prof:\n",
    "        for _ in range(_N_ITERATIONS):\n",
    "            model(input_ids=input)\n",
    "            prof.step()\n",
    "    print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n",
    "\n",
    "def print_tree_profile_stats(input, mask, position_ids, model):\n",
    "    with torch.inference_mode(), profile(**profiler_kwargs) as prof:\n",
    "        for _ in range(_N_ITERATIONS):\n",
    "            model(input_ids=input, attention_mask=mask, position_ids=position_ids)\n",
    "            prof.step()\n",
    "    print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "832d137b-17d8-4813-9e1e-3d4b5d06b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   450, 29871, 29896, 29900],\n",
      "        [    1,   450, 29871, 29896, 29929],\n",
      "        [    1,   450,   937,  2655,   366],\n",
      "        [    1,   450,   937,  2655,   306]], device='cuda:0')\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29929,   937,  2655,   366,   306]],\n",
      "       device='cuda:0') tensor([[[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 1, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 0, 1]]]], device='cuda:0',\n",
      "       dtype=torch.int32) tensor([[0, 1, 2, 3, 4, 4, 2, 3, 4, 4]], device='cuda:0')\n",
      "Sequential Time:  0.029008917570114135\n",
      "Tree Time:  0.029660537242889404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-13 00:01:29 1994710:1994710 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-05-13 00:01:29 1994710:1994710 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-13 00:01:29 1994710:1994710 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               aten::mm         8.77%      22.648ms        13.99%      36.143ms      24.688us     195.666ms        41.24%     200.700ms     137.090us           0 b           0 b     547.66 Mb     547.66 Mb          1464  \n",
      "                                              aten::mul         6.82%      17.623ms        10.22%      26.394ms      13.980us       4.115ms         0.87%       6.353ms       3.365us           0 b           0 b     547.27 Mb     547.27 Mb          1888  \n",
      "                                              aten::add         4.37%      11.299ms         6.62%      17.096ms      13.611us       2.088ms         0.44%       3.361ms       2.676us         -16 b         -16 b     203.33 Mb     203.33 Mb          1256  \n",
      "                                             aten::silu         0.80%       2.069ms         1.23%       3.166ms      15.221us     438.000us         0.09%       1.385ms       6.659us           0 b           0 b     137.11 Mb     137.11 Mb           208  \n",
      "                                              aten::pow         1.88%       4.862ms         2.74%       7.071ms      16.677us     425.000us         0.09%       1.258ms       2.967us           0 b           0 b     103.52 Mb     103.52 Mb           424  \n",
      "                                              aten::cat         2.93%       7.575ms         4.17%      10.780ms      17.276us       2.049ms         0.43%       2.831ms       4.537us           0 b           0 b     101.97 Mb     101.97 Mb           624  \n",
      "                                            aten::empty         0.87%       2.241ms         0.87%       2.241ms       2.643us       0.000us         0.00%       0.000us       0.000us       2.62 Kb       2.62 Kb      50.78 Mb      50.78 Mb           848  \n",
      "                                              aten::neg         1.44%       3.719ms         2.24%       5.785ms      13.906us     773.000us         0.16%       1.382ms       3.322us           0 b           0 b      50.78 Mb      50.78 Mb           416  \n",
      "                                           aten::linear         1.82%       4.710ms        19.70%      50.891ms      34.762us       0.000us         0.00%     193.652ms     132.276us           0 b           0 b     547.66 Mb      20.12 Mb          1464  \n",
      "                                          aten::resize_         0.04%      96.000us         0.04%      96.000us       6.000us       0.000us         0.00%       0.000us       0.000us           0 b           0 b       1.96 Mb       1.96 Mb            16  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 258.343ms\n",
      "Self CUDA time total: 474.422ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-05-13 00:01:33 1994710:1994710 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2024-05-13 00:01:33 1994710:1994710 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-05-13 00:01:33 1994710:1994710 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                               aten::mm         8.17%      23.143ms        13.29%      37.639ms      25.710us     201.013ms        39.95%     201.013ms     137.304us           0 b           0 b     275.06 Mb     275.06 Mb          1464  \n",
      "                                              aten::mul         5.89%      16.688ms         8.94%      25.324ms      13.300us       3.799ms         0.76%       3.799ms       1.995us           0 b           0 b     273.64 Mb     273.64 Mb          1904  \n",
      "                                              aten::add         4.54%      12.859ms         6.58%      18.648ms      14.753us       1.817ms         0.36%       1.817ms       1.438us         -24 b         -24 b     101.77 Mb     101.77 Mb          1264  \n",
      "                                            aten::empty         2.16%       6.110ms         2.16%       6.110ms       3.603us       0.000us         0.00%       0.000us       0.000us       2.68 Kb       2.68 Kb     101.77 Mb     101.77 Mb          1696  \n",
      "                                             aten::silu         0.72%       2.038ms         1.10%       3.103ms      14.918us     416.000us         0.08%     416.000us       2.000us           0 b           0 b      68.55 Mb      68.55 Mb           208  \n",
      "                                              aten::pow         1.70%       4.825ms         2.47%       6.986ms      16.476us     424.000us         0.08%     424.000us       1.000us           0 b           0 b      51.76 Mb      51.76 Mb           424  \n",
      "                                              aten::cat         2.68%       7.580ms         3.80%      10.760ms      17.244us       2.029ms         0.40%       2.029ms       3.252us           0 b           0 b      51.59 Mb      51.59 Mb           624  \n",
      "                                              aten::neg         1.32%       3.741ms         2.74%       7.749ms      18.627us     592.000us         0.12%     592.000us       1.423us           0 b           0 b      25.39 Mb      25.39 Mb           416  \n",
      "                                       aten::contiguous         0.55%       1.544ms         4.35%      12.333ms      19.764us       0.000us         0.00%       1.029ms       1.649us           0 b           0 b      76.17 Mb       9.77 Mb           624  \n",
      "                                           aten::linear         1.62%       4.590ms        18.29%      51.812ms      35.391us       0.000us         0.00%     193.964ms     132.489us           0 b           0 b     275.06 Mb       9.50 Mb          1464  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 283.265ms\n",
      "Self CUDA time total: 503.118ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    token_tree = _create_token_tree(\n",
    "        expansion_config=(2, 1, 2),\n",
    "        prompt=\"The\",\n",
    "        tokenizer=tokenizer,\n",
    "        model=ssm,\n",
    "    )\n",
    "    print(token_tree)\n",
    "\n",
    "    # construct inputs for tree decoding\n",
    "    tree_input, tree_mask, tree_position_ids = construct_tree_model_inputs(token_tree)\n",
    "    print(tree_input, tree_mask, tree_position_ids)\n",
    "    \n",
    "    sequential_time = time_normal(token_tree, N_ITERATIONS, llm)\n",
    "    tree_time = time_tree(tree_input, tree_mask, tree_position_ids, N_ITERATIONS, llm)\n",
    "    print(\"Sequential Time: \", sequential_time)\n",
    "    print(\"Tree Time: \", tree_time)\n",
    "\n",
    "    print_normal_profile_stats(token_tree, llm)\n",
    "    print_tree_profile_stats(tree_input, tree_mask, tree_position_ids, llm)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185eda4d-e415-4bda-afa4-22a5c6b44908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
