{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1883387e-2829-499b-baeb-9e88d7da49fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# from collections.abc import Sequence\n",
    "from typing import Sequence\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "_SSM_NAME = \"JackFram/llama-160m\"\n",
    "_LLM_NAME = 'openlm-research/open_llama_3b_v2'\n",
    "device = \"cuda\"\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "tokenizer = AutoTokenizer.from_pretrained(_SSM_NAME)\n",
    "ssm = AutoModelForCausalLM.from_pretrained(_SSM_NAME).cuda()\n",
    "llm = AutoModelForCausalLM.from_pretrained(_LLM_NAME).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d0fe080-cfbb-4cbb-8d2a-22f3acf2a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_token_tree(\n",
    "    expansion_config: Sequence[int],\n",
    "    prompt: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: AutoModelForCausalLM,\n",
    "    has_kv_cache: bool = False,\n",
    "):\n",
    "    \"\"\"Create token tree following Figure 3 in the paper.\n",
    "\n",
    "    We don't need \"real\" tokens for our experiments - just\n",
    "    random integers would work too - but might as well.\n",
    "\n",
    "    Figure 3 illustrates the <k1, k2, ...> expansion approach they\n",
    "    use to create token trees. We can use each of the top_k tokens from\n",
    "    a single model to create the same tree structure.\n",
    "\n",
    "    Args:\n",
    "        expansion_config: A sequence of integers representing how much to\n",
    "            branch at each generation step.\n",
    "        prompt: Initial prompt.\n",
    "        tokenizer: HF tokenizer.\n",
    "        model: HF generative model.\n",
    "        has_kv_cache: If there are preceding tokens this is generating after, in which\n",
    "            case we exclude the begin-of-sequence token.\n",
    "    \"\"\"\n",
    "    assert expansion_config\n",
    "    current_tree = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    if has_kv_cache:\n",
    "        assert tokenizer.add_bos_token\n",
    "        current_tree = current_tree[:, 1:]\n",
    "    for k in expansion_config:\n",
    "        output = model.generate(\n",
    "            current_tree,\n",
    "            max_new_tokens=1,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "        # Take the top_k tokens from the 1 generation step we've done\n",
    "        top_k = torch.topk(output.scores[-1], k=k, dim=-1).indices.reshape(-1, 1)\n",
    "        current_tree = torch.repeat_interleave(current_tree, k, dim=0)\n",
    "        # Join the top_k tokens to the current tree\n",
    "        current_tree = torch.cat((current_tree, top_k), dim=-1)\n",
    "\n",
    "    return current_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f853eaf-c00b-46b7-8424-7026c0bc8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def _invert_4d_attention_mask(attention_mask: torch.Tensor, kv_cache_num_tokens: int=0) -> torch.Tensor:\n",
    "    \"\"\"For 4D masks, new HF requires us to invert the mask so it doesn't modify it at all.\"\"\"\n",
    "    # The attention mask must have last 2 dims shape [current seq len, KV cache size + current seq len]\n",
    "    # So we prepend a tensor of 1s to allow attending to the full KV cache\n",
    "    assert attention_mask.dim() == 4\n",
    "    if kv_cache_num_tokens > 0:\n",
    "        attention_mask = torch.cat(\n",
    "            (\n",
    "                torch.ones(\n",
    "                    attention_mask.shape[0],\n",
    "                    attention_mask.shape[1],\n",
    "                    attention_mask.shape[2],\n",
    "                    kv_cache_num_tokens,\n",
    "                ).to(device),\n",
    "                attention_mask,\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "    # Invert the mask: 0s to -inf and 1s to 0 (0 means attention allowed)\n",
    "    min_dtype = torch.finfo(torch.float32).min\n",
    "    attention_mask.masked_fill_(attention_mask == 0, min_dtype)\n",
    "    attention_mask.masked_fill_(attention_mask == 1, 0.0)\n",
    "    return attention_mask\n",
    "\n",
    "def construct_tree_model_inputs(sequences):\n",
    "    # input_1 = torch.unique(torch.flatten(sequences), sorted=False)\n",
    "    flat = torch.flatten(sequences).tolist()\n",
    "    unique = []\n",
    "    for tok in flat:\n",
    "        if tok not in unique:\n",
    "            unique.append(tok)\n",
    "    # input is list of unique tokens\n",
    "    input_1 = torch.tensor([unique]).to(device)\n",
    "\n",
    "    a = input_1.shape[-1]\n",
    "    mask_1 = np.zeros((a, a), dtype=np.float32)\n",
    "    positions = [-1] * len(unique)\n",
    "    \n",
    "    for seq in sequences:\n",
    "        branch_progress = []\n",
    "        for (pos, tok) in enumerate(seq):\n",
    "            input_1_idx = unique.index(tok)\n",
    "            positions[input_1_idx] = pos\n",
    "            branch_progress.append(input_1_idx)\n",
    "            for idx in branch_progress:\n",
    "                mask_1[input_1_idx][idx] = 1\n",
    "    mask_1 = torch.tensor(mask_1, device=device)\n",
    "    mask_1 = mask_1.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    position_ids_1 = torch.tensor([positions], device=device, dtype=torch.int64)\n",
    "    return (input_1, mask_1, position_ids_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5614283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_dummy_kv_cache(\n",
    "    kv_cache_num_tokens: int,\n",
    "    batch_size: int,\n",
    "    num_attention_heads: int,\n",
    "    hidden_size: int,\n",
    "    num_layers: int,\n",
    "):\n",
    "    k = torch.rand(\n",
    "        batch_size,\n",
    "        num_attention_heads,\n",
    "        kv_cache_num_tokens,\n",
    "        hidden_size // num_attention_heads,\n",
    "    ).to(device)\n",
    "    v = torch.rand(\n",
    "        batch_size,\n",
    "        num_attention_heads,\n",
    "        kv_cache_num_tokens,\n",
    "        hidden_size // num_attention_heads,\n",
    "    ).to(device)\n",
    "    return tuple((k, v) for _ in range(num_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "725b432b",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_LLM_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;66;43;03m#    torch_dtype=torch.float16,\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                          \u001b[49m\u001b[38;5;66;43;03m#  attn_implementation=\"sdpa\"\u001b[39;49;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43m                                           \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/transformers/modeling_utils.py:2694\u001b[0m, in \u001b[0;36mPreTrainedModel.cuda\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2689\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2690\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalling `cuda()` is not supported for `4-bit` or `8-bit` quantized models. Please use the model as it is, since the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2691\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m model has already been set to the correct devices and casted to the correct `dtype`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2692\u001b[0m     )\n\u001b[1;32m   2693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule.cuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    899\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \n\u001b[1;32m    901\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 779 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/torch/nn/modules/module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/torch/nn/modules/module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/torch/nn/modules/module.py:915\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    899\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Move all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[1;32m    900\u001b[0m \n\u001b[1;32m    901\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "llm = AutoModelForCausalLM.from_pretrained(_LLM_NAME,\n",
    "                                        #    torch_dtype=torch.float16,\n",
    "                                          #  attn_implementation=\"sdpa\"\n",
    "                                           ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3de2107-a040-4a06-8b3d-8b0e268a43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_normal(input_ids, model: AutoModelForCausalLM, kv_cache=None):\n",
    "    with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True, enable_cudnn=False):\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16), torch.inference_mode():\n",
    "            model(input_ids=input_ids, past_key_values=kv_cache, use_cache=kv_cache is not None)\n",
    "\n",
    "def time_tree(input_ids, mask, position_ids, model: AutoModelForCausalLM, kv_cache=None):\n",
    "    with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True, enable_cudnn=False):\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16), torch.inference_mode():\n",
    "            model(input_ids=input_ids, attention_mask=mask, position_ids=position_ids, past_key_values=kv_cache, use_cache=kv_cache is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e0930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, ProfilerActivity, schedule\n",
    "\n",
    "# Guide: https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n",
    "\n",
    "_N_ITERATIONS = 10\n",
    "_WAIT_STEPS = 1\n",
    "_WARMUP_STEPS = 1\n",
    "schedule_params = {\n",
    "    'wait': _WAIT_STEPS,\n",
    "    'warmup': _WARMUP_STEPS,\n",
    "    'active': _N_ITERATIONS - _WAIT_STEPS - _WARMUP_STEPS,\n",
    "}\n",
    "profiler_kwargs = {\n",
    "    'activities': [ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    'profile_memory': True,\n",
    "    'schedule': schedule(**schedule_params),\n",
    "    'record_shapes': True,\n",
    "    'with_stack': True,\n",
    "    'on_trace_ready': torch.profiler.tensorboard_trace_handler('./log'),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def print_normal_profile_stats(input, model):\n",
    "    with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True, enable_cudnn=False):\n",
    "    with torch.inference_mode(), profile(**profiler_kwargs) as prof:\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            for _ in range(_N_ITERATIONS):\n",
    "                model(input_ids=input)\n",
    "                prof.step()\n",
    "    # print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n",
    "\n",
    "def print_tree_profile_stats(input, mask, position_ids, model):\n",
    "    # with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True, enable_cudnn=False):\n",
    "    with torch.inference_mode(), profile(**profiler_kwargs) as prof:\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            for _ in range(_N_ITERATIONS):\n",
    "                model(input_ids=input, attention_mask=mask, position_ids=position_ids)\n",
    "                prof.step()\n",
    "    # print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d04c551a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "past key values None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-06-02 18:52:35 905165:905165 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "past key values None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-06-02 18:52:35 905165:905165 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2024-06-02 18:52:35 905165:905165 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "expansion_config = (7, 1, 1, 1, 1, 1, 1, 1, 1)\n",
    "\n",
    "# del llm\n",
    "# llm = AutoModelForCausalLM.from_pretrained(_LLM_NAME).cuda().eval()\n",
    "token_tree = _create_token_tree(\n",
    "        expansion_config=expansion_config,\n",
    "        prompt=\"The\",\n",
    "        tokenizer=tokenizer,\n",
    "        model=ssm,\n",
    ")\n",
    "\n",
    "print_normal_profile_stats(token_tree, llm)\n",
    "\n",
    "# tree_input, tree_mask, tree_position_ids = construct_tree_model_inputs(token_tree)\n",
    "# tree_mask = _invert_4d_attention_mask(tree_mask)\n",
    "# print_tree_profile_stats(tree_input, tree_mask, tree_position_ids, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95e6a132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 11])\n"
     ]
    }
   ],
   "source": [
    "print(token_tree.shape)\n",
    "# tree_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b13b2ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Optionally use the context manager to ensure one of the fused kernels is run\n",
    "import torch.nn.functional as F\n",
    "query = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
    "key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
    "value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
    "mask = torch.rand(32, 1, 128, 128, dtype=torch.float16, device=\"cuda\")\n",
    "with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True, enable_cudnn=False):\n",
    "    F.scaled_dot_product_attention(query,key,value, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c70e83e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import utils\n",
    "\n",
    "utils.is_flash_attn_2_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a5a17ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "params = torch.backends.cuda.SDPAParams(\n",
    "    torch.ones(1, 1, 3, 3, dtype=torch.float16).cuda(),\n",
    "    torch.ones(1, 1, 3, 3, dtype=torch.float16).cuda(),\n",
    "    torch.ones(1, 1, 3, 3, dtype=torch.float16).cuda(),\n",
    "    # None,\n",
    "    torch.ones(1, 1, 3, 3, dtype=torch.float16).cuda(),\n",
    "    0.0,\n",
    "    False\n",
    ")\n",
    "\n",
    "torch.backends.cuda.can_use_efficient_attention(params, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d28f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure max memory allocated\n",
    "import gc\n",
    "\n",
    "def reset_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "def end_memory_collection():\n",
    "    torch.cuda.synchronize()\n",
    "    max_mem_gb = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    return max_mem_gb\n",
    "\n",
    "token_tree = _create_token_tree([2, 2, 2], \"The\", tokenizer, ssm)\n",
    "reset_memory()\n",
    "time_normal(token_tree, llm, kv_cache=None)\n",
    "seq_max_mem_gb = end_memory_collection()\n",
    "\n",
    "tree_input, tree_mask, tree_position_ids = construct_tree_model_inputs(token_tree)\n",
    "tree_mask = _invert_4d_attention_mask(tree_mask)\n",
    "reset_memory()\n",
    "time_tree(input_ids=tree_input, mask=tree_mask, position_ids=tree_position_ids, model=llm, kv_cache=None)\n",
    "tree_max_mem_gb = end_memory_collection()\n",
    "\n",
    "print(f\"Normal: {seq_max_mem_gb:.2f} GB\")\n",
    "print(f\"Tree: {tree_max_mem_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "832d137b-17d8-4813-9e1e-3d4b5d06b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29900, 29995, 20118],\n",
      "        [    1,   450,   937,  2655,   366,   881,   437,   338],\n",
      "        [    1,   450,  1900,   982,   304,   679,  4687,   338],\n",
      "        [    1,   450,  1353,   310,  2305,  1058,   505,  1063],\n",
      "        [    1,   450,  1556,  4100,  2655,   338,   304,   367],\n",
      "        [    1,   450,   315, 14044,   261, 29892,   315, 14044],\n",
      "        [    1,   450,   349,   682,   383, 18966, 30010, 29879],\n",
      "        [    1,   450,   716,   342,  6124,   304,   278,  3942],\n",
      "        [    1,   450,  1833,   931,   306,   471,   297,   278],\n",
      "        [    1,   450,   350, 25365, 29871, 29941, 10488, 29892],\n",
      "        [    1,   450,   402, 26265,   310,  2259, 29889,    13],\n",
      "        [    1,   450,  1021,   931, 29892,   278,  5001,   756],\n",
      "        [    1,   450,  5001,   338,   263,  8236, 13113,   310],\n",
      "        [    1,   450,  2446,  2462, 29892,   306,  3512,   304],\n",
      "        [    1,   450,   390,   895,   310,   278, 29871, 29896],\n",
      "        [    1,   450,   916,  2462, 29892,   306,   471,   297]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   450, 29871, 29896, 29900, 29995, 20118,   937,  2655,   366,\n",
      "           881,   437,   338,  1900,   982,   304,   679,  4687,  1353,   310,\n",
      "          2305,  1058,   505,  1063,  1556,  4100,   367,   315, 14044,   261,\n",
      "         29892,   349,   682,   383, 18966, 30010, 29879,   716,   342,  6124,\n",
      "           278,  3942,  1833,   931,   306,   471,   297,   350, 25365, 29941,\n",
      "         10488,   402, 26265,  2259, 29889,    13,  1021,  5001,   756,   263,\n",
      "          8236, 13113,  2446,  2462,  3512,   390,   895,   916]],\n",
      "       device='cuda:0') tensor([[[[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 0.],\n",
      "          ...,\n",
      "          [1., 1., 0.,  ..., 1., 0., 0.],\n",
      "          [1., 1., 0.,  ..., 1., 1., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 0., 1.]]]], device='cuda:0') tensor([[0, 1, 6, 7, 5, 6, 7, 2, 4, 4, 5, 6, 3, 2, 3, 7, 5, 6, 2, 4, 4, 5, 6, 7,\n",
      "         2, 3, 7, 6, 7, 4, 4, 2, 3, 4, 5, 6, 7, 2, 3, 4, 5, 7, 2, 3, 5, 6, 7, 2,\n",
      "         3, 5, 6, 2, 3, 5, 6, 7, 2, 2, 7, 4, 5, 6, 2, 3, 6, 2, 3, 2]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "import numpy as np\n",
    "\n",
    "N_ITERATIONS = 32\n",
    "\n",
    "tree_widths = [4] #range(1, 4)\n",
    "sequential_times = []\n",
    "tree_times = []\n",
    "\n",
    "kv_cache_num_tokens = 128\n",
    "\n",
    "for tree_width in tree_widths:\n",
    "    expansion_config = (16, 1, 1, 1, 1, 1)\n",
    "\n",
    "    token_tree = _create_token_tree(\n",
    "        expansion_config=expansion_config,\n",
    "        prompt=\"The\",\n",
    "        tokenizer=tokenizer,\n",
    "        model=ssm,\n",
    "    )\n",
    "    print(token_tree)\n",
    "\n",
    "    batch_size=np.prod(expansion_config)\n",
    "    # kv_cache_sequential = _create_dummy_kv_cache(\n",
    "    #     kv_cache_num_tokens=kv_cache_num_tokens,\n",
    "    #     batch_size=batch_size,\n",
    "    #     num_attention_heads=llm.config.num_attention_heads,\n",
    "    #     hidden_size=llm.config.hidden_size,\n",
    "    #     num_layers=llm.config.num_hidden_layers\n",
    "    # )\n",
    "    \n",
    "    sequential_timer = benchmark.Timer(\n",
    "        stmt=\"time_normal(input_ids, model, kv_cache)\",\n",
    "        setup=\"from __main__ import time_normal\",\n",
    "        num_threads=torch.get_num_threads(),\n",
    "        globals={\n",
    "            'input_ids': token_tree,\n",
    "            'model': llm,\n",
    "            'kv_cache': None\n",
    "        },\n",
    "        label=\"Sequential\"\n",
    "    )\n",
    "    sequential_measurement = sequential_timer.timeit(N_ITERATIONS)\n",
    "    sequential_times.append(sequential_measurement.times[-1])\n",
    "    \n",
    "    # construct inputs for tree decoding\n",
    "    # kv_cache_tree = _create_dummy_kv_cache(\n",
    "    #     kv_cache_num_tokens=kv_cache_num_tokens,\n",
    "    #     batch_size=1,\n",
    "    #     num_attention_heads=llm.config.num_attention_heads,\n",
    "    #     hidden_size=llm.config.hidden_size,\n",
    "    #     num_layers=llm.config.num_hidden_layers\n",
    "    # )\n",
    "    tree_input, tree_mask, tree_position_ids = construct_tree_model_inputs(token_tree)\n",
    "    print(tree_input, tree_mask, tree_position_ids)\n",
    "    # Required for 4D mask support in new HF\n",
    "    tree_mask = _invert_4d_attention_mask(tree_mask, kv_cache_num_tokens)\n",
    "\n",
    "    tree_timer = benchmark.Timer(\n",
    "        stmt=\"time_tree(input_ids, mask, position_ids, model, kv_cache)\",\n",
    "        setup=\"from __main__ import time_tree\",\n",
    "        num_threads=torch.get_num_threads(),\n",
    "        globals={\n",
    "            'input_ids': tree_input,\n",
    "            'mask': tree_mask,\n",
    "            'position_ids': tree_position_ids,\n",
    "            'model': llm,\n",
    "            'kv_cache': None\n",
    "        },\n",
    "        label=\"Tree\"\n",
    "    )\n",
    "    tree_measurement = tree_timer.timeit(N_ITERATIONS)\n",
    "    tree_times.append(tree_measurement.times[-1])\n",
    "    \n",
    "    # print_normal_profile_stats(token_tree, llm)\n",
    "    # print_tree_profile_stats(tree_input, tree_mask, tree_position_ids, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75554c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "width = 0.35\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "x_data = tree_widths\n",
    "y_sequential = np.array(sequential_times) * 1000 # scale to ms\n",
    "plt.bar(x_data, y_sequential, label=\"Sequential\", width=width)  # Plot the first list as the y-axis values\n",
    "y_tree = np.array(tree_times) * 1000 # scale to ms\n",
    "plt.bar([pos + width for pos in x_data], y_tree, label=\"Tree\", width=width)  # Plot the second list as the y-axis values\n",
    "\n",
    "plt.xlabel(\"Tree Width\")\n",
    "plt.ylabel(\"Latency (ms)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(\"tree_vs_sequential.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cae5d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory-bound: arithmetic intensity 7.448872654272583 < 27.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import metrics\n",
    "from transformers import AutoConfig\n",
    "\n",
    "mistral_7b_config = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "metrics.identify_compute_memory_bound(\n",
    "    gpu=metrics.T4,\n",
    "    token_batch=torch.ones(1, np.sum(np.cumprod(expansion_config))),\n",
    "    dtype=torch.float32,\n",
    "    num_layers=mistral_7b_config.num_hidden_layers,\n",
    "    d_model=mistral_7b_config.hidden_size,\n",
    "    n_head=mistral_7b_config.num_attention_heads,\n",
    "    vocab_size=mistral_7b_config.vocab_size,\n",
    "    kv_cache_token_count=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
