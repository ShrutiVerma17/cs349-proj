{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1883387e-2829-499b-baeb-9e88d7da49fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# from collections.abc import Sequence\n",
    "from typing import Sequence\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "_SSM_NAME = \"JackFram/llama-160m\"\n",
    "_LLM_NAME = 'openlm-research/open_llama_3b_v2'\n",
    "device = \"cuda\"\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "tokenizer = AutoTokenizer.from_pretrained(_SSM_NAME)\n",
    "ssm = AutoModelForCausalLM.from_pretrained(_SSM_NAME).cuda()\n",
    "llm = AutoModelForCausalLM.from_pretrained(_LLM_NAME).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d0fe080-cfbb-4cbb-8d2a-22f3acf2a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_token_tree(\n",
    "    expansion_config: Sequence[int],\n",
    "    prompt: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: AutoModelForCausalLM,\n",
    "):\n",
    "    \"\"\"Create token tree following Figure 3 in the paper.\n",
    "\n",
    "    We don't need \"real\" tokens for our experiments - just\n",
    "    random integers would work too - but might as well.\n",
    "\n",
    "    Figure 3 illustrates the <k1, k2, ...> expansion approach they\n",
    "    use to create token trees. We can use each of the top_k tokens from\n",
    "    a single model to create the same tree structure.\n",
    "\n",
    "    Args:\n",
    "        expansion_config: A sequence of integers representing how much to\n",
    "            branch at each generation step.\n",
    "        prompt: Initial prompt.\n",
    "        tokenizer: HF tokenizer.\n",
    "        model: HF generative model.\n",
    "    \"\"\"\n",
    "    assert expansion_config\n",
    "    current_tree = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "    for k in expansion_config:\n",
    "        output = model.generate(\n",
    "            current_tree,\n",
    "            max_new_tokens=1,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "        # Take the top_k tokens from the 1 generation step we've done\n",
    "        top_k = torch.topk(output.scores[-1], k=k, dim=-1).indices.reshape(-1, 1)\n",
    "        current_tree = torch.repeat_interleave(current_tree, k, dim=0)\n",
    "        # Join the top_k tokens to the current tree\n",
    "        current_tree = torch.cat((current_tree, top_k), dim=-1)\n",
    "\n",
    "    return current_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f853eaf-c00b-46b7-8424-7026c0bc8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tree_model_inputs(sequences):\n",
    "    # input_1 = torch.unique(torch.flatten(sequences), sorted=False)\n",
    "    flat = torch.flatten(sequences).tolist()\n",
    "    unique = []\n",
    "    for tok in flat:\n",
    "        if tok not in unique:\n",
    "            unique.append(tok)\n",
    "    # input is list of unique tokens\n",
    "    input_1 = torch.tensor([unique]).to(device)\n",
    "\n",
    "    a = input_1.shape[-1]\n",
    "    mask_1 = np.zeros((a, a))\n",
    "    positions = [-1] * len(unique)\n",
    "    \n",
    "    for seq in sequences:\n",
    "        branch_progress = []\n",
    "        for (pos, tok) in enumerate(seq):\n",
    "            input_1_idx = unique.index(tok)\n",
    "            positions[input_1_idx] = pos\n",
    "            branch_progress.append(input_1_idx)\n",
    "            for idx in branch_progress:\n",
    "                mask_1[input_1_idx][idx] = 1\n",
    "    mask_1 = torch.tensor(mask_1, device=device, dtype=torch.int64)\n",
    "    mask_1 = mask_1.unsqueeze(0).unsqueeze(0).to(device).int()\n",
    "    position_ids_1 = torch.tensor([positions], device=device, dtype=torch.int64)\n",
    "    return (input_1, mask_1, position_ids_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3de2107-a040-4a06-8b3d-8b0e268a43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_normal(input_ids, model: AutoModelForCausalLM):\n",
    "    with torch.inference_mode():\n",
    "        model(input_ids=input_ids)\n",
    "\n",
    "def time_tree(input_ids, mask, position_ids, model: AutoModelForCausalLM):\n",
    "    with torch.inference_mode():\n",
    "        model(input_ids=input_ids, attention_mask=mask, position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9e0930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, ProfilerActivity, schedule\n",
    "\n",
    "# Guide: https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n",
    "\n",
    "_N_ITERATIONS = 10\n",
    "_WAIT_STEPS = 1\n",
    "_WARMUP_STEPS = 1\n",
    "schedule_params = {\n",
    "    'wait': _WAIT_STEPS,\n",
    "    'warmup': _WARMUP_STEPS,\n",
    "    'active': _N_ITERATIONS - _WAIT_STEPS - _WARMUP_STEPS,\n",
    "}\n",
    "profiler_kwargs = {\n",
    "    'activities': [ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    'profile_memory': True,\n",
    "    'schedule': schedule(**schedule_params),\n",
    "    'record_shapes': True\n",
    "}\n",
    "\n",
    "def print_normal_profile_stats(input, model):\n",
    "    with torch.inference_mode(), profile(**profiler_kwargs) as prof:\n",
    "        for _ in range(_N_ITERATIONS):\n",
    "            model(input_ids=input)\n",
    "            prof.step()\n",
    "    print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n",
    "\n",
    "def print_tree_profile_stats(input, mask, position_ids, model):\n",
    "    with torch.inference_mode(), profile(**profiler_kwargs) as prof:\n",
    "        for _ in range(_N_ITERATIONS):\n",
    "            model(input_ids=input, attention_mask=mask, position_ids=position_ids)\n",
    "            prof.step()\n",
    "    print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "832d137b-17d8-4813-9e1e-3d4b5d06b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   450, 29871, 29896, 29900, 29900, 29995]], device='cuda:0')\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29995]], device='cuda:0') tensor([[[[1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 0],\n",
      "          [1, 1, 1, 1, 1, 1]]]], device='cuda:0', dtype=torch.int32) tensor([[0, 1, 2, 3, 5, 6]], device='cuda:0')\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29900, 29995],\n",
      "        [    1,   450,   937,  2655,   366,   881,   437]], device='cuda:0')\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29995,   937,  2655,   366,   881,\n",
      "           437]], device='cuda:0') tensor([[[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1]]]], device='cuda:0',\n",
      "       dtype=torch.int32) tensor([[0, 1, 2, 3, 5, 6, 2, 3, 4, 5, 6]], device='cuda:0')\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29900, 29995],\n",
      "        [    1,   450,   937,  2655,   366,   881,   437],\n",
      "        [    1,   450,  1900,   982,   304,   679,  4687]], device='cuda:0')\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29995,   937,  2655,   366,   881,\n",
      "           437,  1900,   982,   304,   679,  4687]], device='cuda:0') tensor([[[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]]], device='cuda:0',\n",
      "       dtype=torch.int32) tensor([[0, 1, 2, 3, 5, 6, 2, 3, 4, 5, 6, 2, 3, 4, 5, 6]], device='cuda:0')\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29900, 29995],\n",
      "        [    1,   450,   937,  2655,   366,   881,   437],\n",
      "        [    1,   450,  1900,   982,   304,   679,  4687],\n",
      "        [    1,   450,  1353,   310,  2305,  1058,   505]], device='cuda:0')\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29995,   937,  2655,   366,   881,\n",
      "           437,  1900,   982,   304,   679,  4687,  1353,   310,  2305,  1058,\n",
      "           505]], device='cuda:0') tensor([[[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]]]],\n",
      "       device='cuda:0', dtype=torch.int32) tensor([[0, 1, 2, 3, 5, 6, 2, 3, 4, 5, 6, 2, 3, 4, 5, 6, 2, 3, 4, 5, 6]],\n",
      "       device='cuda:0')\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29900, 29995],\n",
      "        [    1,   450,   937,  2655,   366,   881,   437],\n",
      "        [    1,   450,  1900,   982,   304,   679,  4687],\n",
      "        [    1,   450,  1353,   310,  2305,  1058,   505],\n",
      "        [    1,   450,  1556,  4100,  2655,   338,   304]], device='cuda:0')\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29995,   937,  2655,   366,   881,\n",
      "           437,  1900,   982,   304,   679,  4687,  1353,   310,  2305,  1058,\n",
      "           505,  1556,  4100,   338]], device='cuda:0') tensor([[[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "           1],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "           0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "           1]]]], device='cuda:0', dtype=torch.int32) tensor([[0, 1, 2, 3, 5, 6, 2, 4, 4, 5, 6, 2, 3, 6, 5, 6, 2, 3, 4, 5, 6, 2, 3, 5]],\n",
      "       device='cuda:0')\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29900, 29995],\n",
      "        [    1,   450,   937,  2655,   366,   881,   437],\n",
      "        [    1,   450,  1900,   982,   304,   679,  4687],\n",
      "        [    1,   450,  1353,   310,  2305,  1058,   505],\n",
      "        [    1,   450,  1556,  4100,  2655,   338,   304],\n",
      "        [    1,   450,   315, 14044,   261, 29892,   315]], device='cuda:0')\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29995,   937,  2655,   366,   881,\n",
      "           437,  1900,   982,   304,   679,  4687,  1353,   310,  2305,  1058,\n",
      "           505,  1556,  4100,   338,   315, 14044,   261, 29892]],\n",
      "       device='cuda:0') tensor([[[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "           1, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "           0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "           1, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 1, 1, 1, 1],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 1, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 1, 1, 1, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "           0, 1, 1, 1, 1]]]], device='cuda:0', dtype=torch.int32) tensor([[0, 1, 2, 3, 5, 6, 2, 4, 4, 5, 6, 2, 3, 6, 5, 6, 2, 3, 4, 5, 6, 2, 3, 5,\n",
      "         6, 3, 4, 5]], device='cuda:0')\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29900, 29995],\n",
      "        [    1,   450,   937,  2655,   366,   881,   437],\n",
      "        [    1,   450,  1900,   982,   304,   679,  4687],\n",
      "        [    1,   450,  1353,   310,  2305,  1058,   505],\n",
      "        [    1,   450,  1556,  4100,  2655,   338,   304],\n",
      "        [    1,   450,   315, 14044,   261, 29892,   315],\n",
      "        [    1,   450,   349,   682,   383, 18966, 30010]], device='cuda:0')\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29995,   937,  2655,   366,   881,\n",
      "           437,  1900,   982,   304,   679,  4687,  1353,   310,  2305,  1058,\n",
      "           505,  1556,  4100,   338,   315, 14044,   261, 29892,   349,   682,\n",
      "           383, 18966, 30010]], device='cuda:0') tensor([[[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 0,  ..., 1, 0, 0],\n",
      "          [1, 1, 0,  ..., 1, 1, 0],\n",
      "          [1, 1, 0,  ..., 1, 1, 1]]]], device='cuda:0', dtype=torch.int32) tensor([[0, 1, 2, 3, 5, 6, 2, 4, 4, 5, 6, 2, 3, 6, 5, 6, 2, 3, 4, 5, 6, 2, 3, 5,\n",
      "         6, 3, 4, 5, 2, 3, 4, 5, 6]], device='cuda:0')\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29900, 29995],\n",
      "        [    1,   450,   937,  2655,   366,   881,   437],\n",
      "        [    1,   450,  1900,   982,   304,   679,  4687],\n",
      "        [    1,   450,  1353,   310,  2305,  1058,   505],\n",
      "        [    1,   450,  1556,  4100,  2655,   338,   304],\n",
      "        [    1,   450,   315, 14044,   261, 29892,   315],\n",
      "        [    1,   450,   349,   682,   383, 18966, 30010],\n",
      "        [    1,   450,   716,   342,  6124,   304,   278]], device='cuda:0')\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29995,   937,  2655,   366,   881,\n",
      "           437,  1900,   982,   304,   679,  4687,  1353,   310,  2305,  1058,\n",
      "           505,  1556,  4100,   338,   315, 14044,   261, 29892,   349,   682,\n",
      "           383, 18966, 30010,   716,   342,  6124,   278]], device='cuda:0') tensor([[[[1, 0, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 0,  ..., 0, 0, 0],\n",
      "          [1, 1, 1,  ..., 0, 0, 0],\n",
      "          ...,\n",
      "          [1, 1, 0,  ..., 1, 0, 0],\n",
      "          [1, 1, 0,  ..., 1, 1, 0],\n",
      "          [1, 1, 0,  ..., 1, 1, 1]]]], device='cuda:0', dtype=torch.int32) tensor([[0, 1, 2, 3, 5, 6, 2, 4, 4, 5, 6, 2, 3, 5, 5, 6, 2, 3, 4, 5, 6, 2, 3, 5,\n",
      "         6, 3, 4, 5, 2, 3, 4, 5, 6, 2, 3, 4, 6]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "N_ITERATIONS = 32\n",
    "\n",
    "tree_widths = range(1, 9)\n",
    "sequential_times = []\n",
    "tree_times = []\n",
    "\n",
    "for tree_width in tree_widths:\n",
    "    token_tree = _create_token_tree(\n",
    "        expansion_config=(tree_width, 1, 1, 1, 1),\n",
    "        prompt=\"The\",\n",
    "        tokenizer=tokenizer,\n",
    "        model=ssm,\n",
    "    )\n",
    "    print(token_tree)\n",
    "    \n",
    "    sequential_timer = benchmark.Timer(\n",
    "        stmt=\"time_normal(input_ids, model)\",\n",
    "        setup=\"from __main__ import time_normal\",\n",
    "        num_threads=torch.get_num_threads(),\n",
    "        globals={\n",
    "            'input_ids': token_tree,\n",
    "            'model': llm\n",
    "        },\n",
    "        label=\"Sequential\"\n",
    "    )\n",
    "    sequential_measurement = sequential_timer.timeit(N_ITERATIONS)\n",
    "    sequential_times.append(sequential_measurement.times[-1])\n",
    "    \n",
    "    # construct inputs for tree decoding\n",
    "    tree_input, tree_mask, tree_position_ids = construct_tree_model_inputs(token_tree)\n",
    "    print(tree_input, tree_mask, tree_position_ids)\n",
    "\n",
    "    tree_timer = benchmark.Timer(\n",
    "        stmt=\"time_tree(input_ids, mask, position_ids, model)\",\n",
    "        setup=\"from __main__ import time_tree\",\n",
    "        num_threads=torch.get_num_threads(),\n",
    "        globals={\n",
    "            'input_ids': tree_input,\n",
    "            'mask': tree_mask,\n",
    "            'position_ids': tree_position_ids,\n",
    "            'model': llm\n",
    "        },\n",
    "        label=\"Tree\"\n",
    "    )\n",
    "    tree_measurement = tree_timer.timeit(N_ITERATIONS)\n",
    "    tree_times.append(tree_measurement.times[-1])\n",
    "    \n",
    "    # print_normal_profile_stats(token_tree, llm)\n",
    "    # print_tree_profile_stats(tree_input, tree_mask, tree_position_ids, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "75554c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "width = 0.35\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "x_data = tree_widths\n",
    "y_sequential = np.array(sequential_times) * 1000 # scale to ms\n",
    "plt.bar(x_data, y_sequential, label=\"Sequential\", width=width)  # Plot the first list as the y-axis values\n",
    "y_tree = np.array(tree_times) * 1000 # scale to ms\n",
    "plt.bar([pos + width for pos in x_data], y_tree, label=\"Tree\", width=width)  # Plot the second list as the y-axis values\n",
    "\n",
    "plt.xlabel(\"Tree Width\")\n",
    "plt.ylabel(\"Latency (ms)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(\"tree_vs_sequential.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b0f0273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.51.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (159 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m159.5/159.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.21 in /nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages (from matplotlib) (24.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-10.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (9.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages (from matplotlib) (2.9.0)\n",
      "Requirement already satisfied: six>=1.5 in /nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Downloading matplotlib-3.8.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (309 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.2/309.2 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.51.0-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m49.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached kiwisolver-1.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
      "Downloading pillow-10.3.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.2/103.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.2.1 cycler-0.12.1 fonttools-4.51.0 kiwisolver-1.4.5 matplotlib-3.8.4 pillow-10.3.0 pyparsing-3.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c2604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
