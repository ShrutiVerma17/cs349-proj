{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1883387e-2829-499b-baeb-9e88d7da49fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# from collections.abc import Sequence\n",
    "from typing import Sequence\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "_SSM_NAME = \"JackFram/llama-160m\"\n",
    "_LLM_NAME = 'openlm-research/open_llama_3b_v2'\n",
    "device = \"cuda\"\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "tokenizer = AutoTokenizer.from_pretrained(_SSM_NAME)\n",
    "ssm = AutoModelForCausalLM.from_pretrained(_SSM_NAME).cuda()\n",
    "llm = AutoModelForCausalLM.from_pretrained(_LLM_NAME, torch_dtype=torch.bfloat16).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d0fe080-cfbb-4cbb-8d2a-22f3acf2a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_token_tree(\n",
    "    expansion_config: Sequence[int],\n",
    "    prompt: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: AutoModelForCausalLM,\n",
    "    has_kv_cache: bool = False,\n",
    "):\n",
    "    \"\"\"Create token tree following Figure 3 in the paper.\n",
    "\n",
    "    We don't need \"real\" tokens for our experiments - just\n",
    "    random integers would work too - but might as well.\n",
    "\n",
    "    Figure 3 illustrates the <k1, k2, ...> expansion approach they\n",
    "    use to create token trees. We can use each of the top_k tokens from\n",
    "    a single model to create the same tree structure.\n",
    "\n",
    "    Args:\n",
    "        expansion_config: A sequence of integers representing how much to\n",
    "            branch at each generation step.\n",
    "        prompt: Initial prompt.\n",
    "        tokenizer: HF tokenizer.\n",
    "        model: HF generative model.\n",
    "    \"\"\"\n",
    "    assert expansion_config\n",
    "    current_tree = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "    if has_kv_cache:\n",
    "        assert tokenizer.add_bos_token\n",
    "        current_tree = current_tree[:, 1:]\n",
    "    else:\n",
    "        current_tree = current_tree[:, :-1]\n",
    "    # assert current_tree.shape[-1] == 4\n",
    "    for k in expansion_config:\n",
    "        output = model.generate(\n",
    "            current_tree,\n",
    "            max_new_tokens=1,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "        # Take the top_k tokens from the 1 generation step we've done\n",
    "        top_k = torch.topk(output.scores[-1], k=k, dim=-1).indices.reshape(-1, 1)\n",
    "        current_tree = torch.repeat_interleave(current_tree, k, dim=0)\n",
    "        # Join the top_k tokens to the current tree\n",
    "        current_tree = torch.cat((current_tree, top_k), dim=-1)\n",
    "\n",
    "    return current_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f853eaf-c00b-46b7-8424-7026c0bc8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def _invert_4d_attention_mask(attention_mask: torch.Tensor, kv_cache_num_tokens: int=0) -> torch.Tensor:\n",
    "    \"\"\"For 4D masks, new HF requires us to invert the mask so it doesn't modify it at all.\"\"\"\n",
    "    # The attention mask must have last 2 dims shape [current seq len, KV cache size + current seq len]\n",
    "    # So we prepend a tensor of 1s to allow attending to the full KV cache\n",
    "    assert attention_mask.dim() == 4\n",
    "    if kv_cache_num_tokens > 0:\n",
    "        attention_mask = torch.cat(\n",
    "            (\n",
    "                torch.ones(\n",
    "                    attention_mask.shape[0],\n",
    "                    attention_mask.shape[1],\n",
    "                    attention_mask.shape[2],\n",
    "                    kv_cache_num_tokens,\n",
    "                ).to(device),\n",
    "                attention_mask,\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "    # Invert the mask: 0s to -inf and 1s to 0 (0 means attention allowed)\n",
    "    min_dtype = torch.finfo(torch.float32).min\n",
    "    attention_mask.masked_fill_(attention_mask == 0, min_dtype)\n",
    "    attention_mask.masked_fill_(attention_mask == 1, 0.0)\n",
    "    return attention_mask\n",
    "\n",
    "def construct_tree_model_inputs(sequences):\n",
    "    # input_1 = torch.unique(torch.flatten(sequences), sorted=False)\n",
    "    flat = torch.flatten(sequences).tolist()\n",
    "    unique = []\n",
    "    for tok in flat:\n",
    "        if tok not in unique:\n",
    "            unique.append(tok)\n",
    "    # input is list of unique tokens\n",
    "    input_1 = torch.tensor([unique]).to(device)\n",
    "\n",
    "    a = input_1.shape[-1]\n",
    "    mask_1 = np.zeros((a, a), dtype=np.float32)\n",
    "    positions = [-1] * len(unique)\n",
    "    \n",
    "    for seq in sequences:\n",
    "        branch_progress = []\n",
    "        for (pos, tok) in enumerate(seq):\n",
    "            input_1_idx = unique.index(tok)\n",
    "            positions[input_1_idx] = pos\n",
    "            branch_progress.append(input_1_idx)\n",
    "            for idx in branch_progress:\n",
    "                mask_1[input_1_idx][idx] = 1\n",
    "    mask_1 = torch.tensor(mask_1, device=device)\n",
    "    mask_1 = mask_1.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    position_ids_1 = torch.tensor([positions], device=device, dtype=torch.int64)\n",
    "    return (input_1, mask_1, position_ids_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5614283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_dummy_kv_cache(\n",
    "    kv_cache_num_tokens: int,\n",
    "    batch_size: int,\n",
    "    num_attention_heads: int,\n",
    "    hidden_size: int,\n",
    "    num_layers: int,\n",
    "):\n",
    "    k = torch.rand(\n",
    "        batch_size,\n",
    "        num_attention_heads,\n",
    "        kv_cache_num_tokens,\n",
    "        hidden_size // num_attention_heads,\n",
    "    ).to(device)\n",
    "    v = torch.rand(\n",
    "        batch_size,\n",
    "        num_attention_heads,\n",
    "        kv_cache_num_tokens,\n",
    "        hidden_size // num_attention_heads,\n",
    "    ).to(device)\n",
    "    return tuple((k, v) for _ in range(num_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3de2107-a040-4a06-8b3d-8b0e268a43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.attention import SDPBackend, sdpa_kernel\n",
    "\n",
    "def time_normal(input_ids, model: AutoModelForCausalLM, kv_cache=None):\n",
    "   with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16), torch.inference_mode():\n",
    "            model(input_ids=input_ids, past_key_values=kv_cache, use_cache=kv_cache is not None)\n",
    "\n",
    "def time_tree(input_ids, mask, position_ids, model: AutoModelForCausalLM, kv_cache=None):\n",
    "    with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16), torch.inference_mode():\n",
    "            model(input_ids=input_ids, attention_mask=mask, position_ids=position_ids, past_key_values=kv_cache, use_cache=kv_cache is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9e0930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, ProfilerActivity, schedule\n",
    "\n",
    "# Guide: https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n",
    "\n",
    "_N_ITERATIONS = 10\n",
    "_WAIT_STEPS = 1\n",
    "_WARMUP_STEPS = 1\n",
    "schedule_params = {\n",
    "    'wait': _WAIT_STEPS,\n",
    "    'warmup': _WARMUP_STEPS,\n",
    "    'active': _N_ITERATIONS - _WAIT_STEPS - _WARMUP_STEPS,\n",
    "}\n",
    "profiler_kwargs = {\n",
    "    'activities': [ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    'profile_memory': True,\n",
    "    'schedule': schedule(**schedule_params),\n",
    "    'record_shapes': True,\n",
    "    'with_stack': True,\n",
    "    'on_trace_ready': torch.profiler.tensorboard_trace_handler('./log'),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def print_normal_profile_stats(input, model):\n",
    "    # with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True, enable_cudnn=False):\n",
    "    with torch.inference_mode(), profile(**profiler_kwargs) as prof:\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            for _ in range(_N_ITERATIONS):\n",
    "                model(input_ids=input)\n",
    "                prof.step()\n",
    "    # print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n",
    "\n",
    "def print_tree_profile_stats(input, mask, position_ids, model):\n",
    "    # with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True, enable_cudnn=False):\n",
    "    with torch.inference_mode(), profile(**profiler_kwargs) as prof:\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            for _ in range(_N_ITERATIONS):\n",
    "                model(input_ids=input, attention_mask=mask, position_ids=position_ids)\n",
    "                prof.step()\n",
    "    # print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d04c551a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:649: UserWarning: Memory efficient kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:607.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:649: UserWarning: Memory Efficient attention has been runtime disabled. (Triggered internally at ../aten/src/ATen/native/transformers/sdp_utils_cpp.h:495.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:649: UserWarning: Flash attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:609.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:649: UserWarning: Both fused kernels do not support non-null attn_mask. (Triggered internally at ../aten/src/ATen/native/transformers/sdp_utils_cpp.h:269.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:649: UserWarning: CuDNN attention kernel not used because: (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:611.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:649: UserWarning: The CuDNN backend needs to be enabled by setting the enviornment variable`TORCH_CUDNN_SDPA_ENABLED=1` (Triggered internally at ../aten/src/ATen/native/transformers/cuda/sdp_utils.cpp:409.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No available kernel. Aborting execution.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m tree_input, tree_mask, tree_position_ids \u001b[38;5;241m=\u001b[39m construct_tree_model_inputs(token_tree)\n\u001b[1;32m     16\u001b[0m tree_mask \u001b[38;5;241m=\u001b[39m _invert_4d_attention_mask(tree_mask)\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtime_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtree_position_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[16], line 11\u001b[0m, in \u001b[0;36mtime_tree\u001b[0;34m(input_ids, mask, position_ids, model, kv_cache)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m sdpa_kernel(SDPBackend\u001b[38;5;241m.\u001b[39mFLASH_ATTENTION):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16), torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[0;32m---> 11\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_cache\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:1164\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1161\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1164\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1170\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1171\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1172\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1177\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:968\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    957\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    958\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    959\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    965\u001b[0m         cache_position,\n\u001b[1;32m    966\u001b[0m     )\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 968\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    978\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:713\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    710\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    712\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 713\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    724\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/transformers/models/llama/modeling_llama.py:649\u001b[0m, in \u001b[0;36mLlamaSdpaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this if statement instead of an\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;66;03m# inline conditional assignment to support both torch.compile's `dynamic=True` and `fullgraph=True`\u001b[39;00m\n\u001b[1;32m    647\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m causal_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m q_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 649\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunctional\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    654\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_dropout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    655\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    656\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    658\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[1;32m    659\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No available kernel. Aborting execution."
     ]
    }
   ],
   "source": [
    "expansion_config = (7, 1, 1, 1, 1, 1, 1, 1, 1)\n",
    "\n",
    "# del llm\n",
    "# llm = AutoModelForCausalLM.from_pretrained(_LLM_NAME).cuda().eval()\n",
    "token_tree = _create_token_tree(\n",
    "        expansion_config=expansion_config,\n",
    "        prompt=\"The\",\n",
    "        tokenizer=tokenizer,\n",
    "        model=ssm,\n",
    ")\n",
    "\n",
    "# time_normal(token_tree, llm)\n",
    "# print_normal_profile_stats(token_tree, llm)\n",
    "\n",
    "tree_input, tree_mask, tree_position_ids = construct_tree_model_inputs(token_tree)\n",
    "tree_mask = _invert_4d_attention_mask(tree_mask)\n",
    "time_tree(tree_input, tree_mask, tree_position_ids, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95e6a132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 11])\n"
     ]
    }
   ],
   "source": [
    "print(token_tree.shape)\n",
    "# tree_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b13b2ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/torch/backends/cuda/__init__.py:342: FutureWarning: torch.backends.cuda.sdp_kernel() is deprecated. In the future, this context manager will be removed. Please see, torch.nn.attention.sdpa_kernel() for the new context manager, with updated signature.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Optionally use the context manager to ensure one of the fused kernels is run\n",
    "import torch.nn.functional as F\n",
    "query = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
    "key = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
    "value = torch.rand(32, 8, 128, 64, dtype=torch.float16, device=\"cuda\")\n",
    "mask = torch.rand(32, 1, 128, 128, dtype=torch.float16, device=\"cuda\")\n",
    "with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=False, enable_math=True, enable_cudnn=False):\n",
    "    F.scaled_dot_product_attention(query,key,value, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c70e83e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import utils\n",
    "\n",
    "utils.is_flash_attn_2_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5a17ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W sdp_utils_cpp.h:269] Warning: Both fused kernels do not support non-null attn_mask. (function check_for_attn_mask)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "params = torch.backends.cuda.SDPAParams(\n",
    "    torch.ones(1, 1, 3, 8, dtype=torch.float16).cuda(),\n",
    "    torch.ones(1, 1, 3, 8, dtype=torch.float16).cuda(),\n",
    "    torch.ones(1, 1, 3, 8, dtype=torch.float16).cuda(),\n",
    "    # None,\n",
    "    torch.ones(1, 1, 3, 3, dtype=torch.float16).cuda(),\n",
    "    0.0,\n",
    "    False\n",
    ")\n",
    "\n",
    "torch.backends.cuda.can_use_flash_attention(params, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f68d28f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "past key values None\n",
      "past key values None\n",
      "past key values None\n",
      "Normal: 19.76176118850708 GB\n",
      "Tree: 19.75322389602661 GB\n"
     ]
    }
   ],
   "source": [
    "# Measure max memory allocated\n",
    "import gc\n",
    "\n",
    "def reset_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "def end_memory_collection():\n",
    "    torch.cuda.synchronize()\n",
    "    max_mem_gb = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    return max_mem_gb\n",
    "\n",
    "token_tree = _create_token_tree([2, 2, 2], \"The good dog is\", tokenizer, ssm)\n",
    "reset_memory()\n",
    "time_normal(token_tree, llm, kv_cache=None)\n",
    "seq_max_mem_gb = end_memory_collection()\n",
    "\n",
    "tree_input, tree_mask, tree_position_ids = construct_tree_model_inputs(token_tree)\n",
    "tree_mask = _invert_4d_attention_mask(tree_mask)\n",
    "reset_memory()\n",
    "time_tree(input_ids=tree_input, mask=tree_mask, position_ids=tree_position_ids, model=llm, kv_cache=None)\n",
    "tree_max_mem_gb = end_memory_collection()\n",
    "\n",
    "print(f\"Normal: {seq_max_mem_gb} GB\")\n",
    "print(f\"Tree: {tree_max_mem_gb} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "832d137b-17d8-4813-9e1e-3d4b5d06b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,    31,   487,   284,   391,  1711, 29892],\n",
      "        [    1,    31,   487,   571,  1705, 29889,   510],\n",
      "        [    1,    31,   598,   261, 29892,   322,   278],\n",
      "        [    1,    31,   598, 29899,   262, 29899, 29874],\n",
      "        [    1,    30, 30088, 30088, 29871, 29906, 29871],\n",
      "        [    1,    30, 30088, 29906, 29906, 29906, 29906],\n",
      "        [    1,    30, 29906, 29941, 29906, 29906, 29906],\n",
      "        [    1,    30, 29906, 29906, 29906, 29889,    13],\n",
      "        [    1,    28, 29879,   413, 29889, 29871, 29945],\n",
      "        [    1,    28, 29879,   310,   593, 29873,   322],\n",
      "        [    1,    28,   645,  1371,  4078,  1371,  4078],\n",
      "        [    1,    28,   645,   599,   367,  4208, 29889],\n",
      "        [    1,    29,  4167,   575,   299, 29879,   299],\n",
      "        [    1,    29,  4167,  4167, 29889,    13,  1576],\n",
      "        [    1,    29, 29879,  4167,  4167,  4167, 29889],\n",
      "        [    1,    29, 29879,  4004,  4004,  4004,  4004],\n",
      "        [    1,    25, 11587,  5867, 29871, 29906, 29889],\n",
      "        [    1,    25, 11587,   351,   601, 29889,    13],\n",
      "        [    1,    25, 28703,  4289, 29892,  9556, 29889],\n",
      "        [    1,    25, 28703, 29874, 29889,    13,  1576],\n",
      "        [    1,    24, 29871, 29896, 29900, 29900, 29900],\n",
      "        [    1,    24, 29871, 29906, 29906, 29906, 29906],\n",
      "        [    1,    24, 30010, 29879, 29871, 29906, 29871],\n",
      "        [    1,    24, 30010, 29871, 29906, 29900, 29896],\n",
      "        [    1,    26, 29906, 29900, 29900, 29900, 29900],\n",
      "        [    1,    26, 29906, 29906, 29906, 29906, 29906],\n",
      "        [    1,    26, 29945, 29945, 29945, 29945, 29906],\n",
      "        [    1,    26, 29945, 29900, 29900, 29900, 29900],\n",
      "        [    1,    27, 29871, 29906, 29906, 29871, 29906],\n",
      "        [    1,    27, 29871, 29946, 29871, 29946, 29871],\n",
      "        [    1,    27,  3436, 29871, 29906, 29906, 29906],\n",
      "        [    1,    27,  3436, 29879,   304,   304,   304],\n",
      "        [    1,    19, 29987, 29987, 29987, 29909, 29987],\n",
      "        [    1,    19, 29987, 30166, 29987, 30166, 29987],\n",
      "        [    1,    19, 30166, 30166, 30166, 30166, 30166],\n",
      "        [    1,    19, 30166, 29987, 30166, 29987, 30166],\n",
      "        [    1,    18,  5296,   310,  5296,   310,  5296],\n",
      "        [    1,    18,  5296,  5296,  5296,  5296,  5296],\n",
      "        [    1,    18, 31681,   510, 29937, 29937, 29937],\n",
      "        [    1,    18, 31681, 31681,   229,   229,   229],\n",
      "        [    1,    16, 11094,   316,   425,   830,  1099],\n",
      "        [    1,    16, 11094, 29892, 29871, 29896, 29929],\n",
      "        [    1,    16, 15759,   264, 29892,   278,  1556],\n",
      "        [    1,    16, 15759,   661, 29892, 29871, 29896],\n",
      "        [    1,    17,   865,   278, 29871, 29896, 29900],\n",
      "        [    1,    17,   865,   368,   411,   278,   916],\n",
      "        [    1,    17, 29885,   681, 29889,    13,  1576],\n",
      "        [    1,    17, 29885, 11925, 29899,  9799, 29892],\n",
      "        [    1,    21, 29987, 29987, 29987, 29987, 29924],\n",
      "        [    1,    21, 29987, 29915, 29987, 29987, 29987],\n",
      "        [    1,    21,  1717,  7114,   515,   515,   515],\n",
      "        [    1,    21,  1717,  1717,  1717,  7395, 29892],\n",
      "        [    1,    20,  2809,  2809,  2809,  2809,  2809],\n",
      "        [    1,    20,  2809, 12627, 12627, 12627, 12627],\n",
      "        [    1,    20, 18567,  3374,   887,  3374,   887],\n",
      "        [    1,    20, 18567, 15043, 21829, 21829, 21829],\n",
      "        [    1,    22, 29955, 29955, 29955, 29955, 29955],\n",
      "        [    1,    22, 29955, 29900, 29900, 29900, 29900],\n",
      "        [    1,    22, 29947, 29947, 29947, 29947, 29947],\n",
      "        [    1,    22, 29947, 29955, 29955, 29955, 29955],\n",
      "        [    1,    23,   915, 29892,   915, 29892,   915],\n",
      "        [    1,    23,   915,   313, 29896, 29896, 29896],\n",
      "        [    1,    23, 29973, 29973, 29973, 29973, 29991],\n",
      "        [    1,    23, 29973, 29991, 29973, 29991, 29973],\n",
      "        [    1,     7, 29871, 29906, 29900, 29900, 29871],\n",
      "        [    1,     7, 29871, 29896, 29900, 29900, 29900],\n",
      "        [    1,     7,   314, 29889, 29889, 29889,   510],\n",
      "        [    1,     7,   314, 29892, 29892, 29892, 29892],\n",
      "        [    1,     6,     6,     6,     6,     6,     6],\n",
      "        [    1,     6,     6, 30098, 30098, 30098, 30098],\n",
      "        [    1,     6, 30098, 30098, 30098, 30098, 30098],\n",
      "        [    1,     6, 30098,   636, 30098, 30098, 30098],\n",
      "        [    1,     4, 29896, 29896, 29896, 29896, 29896],\n",
      "        [    1,     4, 29896, 29906, 29906, 29906, 29906],\n",
      "        [    1,     4, 29941, 29941, 29941, 29941, 29906],\n",
      "        [    1,     4, 29941, 29892, 29871, 29906, 29900],\n",
      "        [    1,     5, 29871, 29871, 29906, 29900, 29896],\n",
      "        [    1,     5, 29871, 29906, 29906, 29906, 29906],\n",
      "        [    1,     5, 30024,  1074,  1074,  1074,  1074],\n",
      "        [    1,     5, 30024, 29929, 29929, 29929, 29929],\n",
      "        [    1,     1,     1,     1,     0,  2167, 28703],\n",
      "        [    1,     1,     1,     0,  2167, 28703,  4289],\n",
      "        [    1,     1,     0,  2167, 29889,    13,  1576],\n",
      "        [    1,     1,     0, 13592, 13592, 13592, 13592],\n",
      "        [    1,     0,  2167,   322,   278,  1791,   310],\n",
      "        [    1,     0,  2167, 29889,    13,  1576,   937],\n",
      "        [    1,     0, 13592, 29891, 29891, 29891, 29891],\n",
      "        [    1,     0, 13592, 13592, 13592, 13592, 13592],\n",
      "        [    1,     2,   322,   278, 29871, 29896, 29900],\n",
      "        [    1,     2,   322,   263, 29871, 29896, 29900],\n",
      "        [    1,     2, 29914, 29879, 29892,   322,   278],\n",
      "        [    1,     2, 29914, 29896, 29900, 29900, 29900],\n",
      "        [    1,     3,     3,     3,     3,     3,     3],\n",
      "        [    1,     3,     3, 29871, 29871, 29871, 29906],\n",
      "        [    1,     3, 29871, 29871, 29906, 29900, 29896],\n",
      "        [    1,     3, 29871, 26308, 26308, 26308, 26308],\n",
      "        [    1,    11, 29896, 29896, 29896, 29896, 29896],\n",
      "        [    1,    11, 29896, 29906, 29896, 29896, 29896],\n",
      "        [    1,    11, 29871, 29871, 29906, 29900, 29896],\n",
      "        [    1,    11, 29871, 29906, 29900, 29896, 29906],\n",
      "        [    1,    10, 19012,   322, 19012,   322, 19012],\n",
      "        [    1,    10, 19012,   592, 19012,   592, 19012],\n",
      "        [    1,    10,    12,    12,    12,  1576, 29871],\n",
      "        [    1,    10,    12, 30119, 30119,  1576, 29871],\n",
      "        [    1,     8,    12,    12,    12,    12,    12],\n",
      "        [    1,     8,    12, 30140, 30140, 30140, 29871],\n",
      "        [    1,     8, 30166, 30166, 30166, 30166, 30166],\n",
      "        [    1,     8, 30166, 30262, 30262, 30262, 30262],\n",
      "        [    1,     9, 29896, 29896, 29896, 29896, 29896],\n",
      "        [    1,     9, 29896, 29906, 29906, 29906, 29906],\n",
      "        [    1,     9, 29906, 29906, 29906, 29906, 29900],\n",
      "        [    1,     9, 29906, 29896, 29896, 29896, 29896],\n",
      "        [    1,    13,  1576, 29871, 29906, 29900, 29896],\n",
      "        [    1,    13,  1576,   937,  2655,   366,   817],\n",
      "        [    1,    13, 29902, 30010, 29885,   451,  1854],\n",
      "        [    1,    13, 29902, 29915, 29885,   451,  1854],\n",
      "        [    1,    12,    12,    12,  1576, 29871, 29906],\n",
      "        [    1,    12,    12,  1576, 29871, 29906, 29900],\n",
      "        [    1,    12,   376,   376, 29924,   520,   310],\n",
      "        [    1,    12,   376,    12,   376,   376, 29924],\n",
      "        [    1,    14, 24052,   837,   264, 29889,    13],\n",
      "        [    1,    14, 24052,  1572, 29892,   278, 29871],\n",
      "        [    1,    14, 24965,   280, 29889,    13,  1576],\n",
      "        [    1,    14, 24965, 29872, 29892, 29871, 29896],\n",
      "        [    1,    15, 26380, 29871, 29896, 29900, 29900],\n",
      "        [    1,    15, 26380,  5240,   861,   267, 29889],\n",
      "        [    1,    15, 21081,  1220, 29889,   510, 29889],\n",
      "        [    1,    15, 21081,  3377,   292, 29889,   510]], device='cuda:0')\n",
      "tensor([[    1,    31,   487,   284,   391,  1711, 29892,   571,  1705, 29889,\n",
      "           510,   598,   261,   322,   278, 29899,   262, 29874,    30, 30088,\n",
      "         29871, 29906, 29941,    13,    28, 29879,   413, 29945,   310,   593,\n",
      "         29873,   645,  1371,  4078,   599,   367,  4208,    29,  4167,   575,\n",
      "           299,  1576,  4004,    25, 11587,  5867,   351,   601, 28703,  4289,\n",
      "          9556,    24, 29896, 29900, 30010,    26,    27, 29946,  3436,   304,\n",
      "            19, 29987, 29909, 30166,    18,  5296, 31681, 29937,   229,    16,\n",
      "         11094,   316,   425,   830,  1099, 29929, 15759,   264,  1556,   661,\n",
      "            17,   865,   368,   411,   916, 29885,   681, 11925,  9799,    21,\n",
      "         29924, 29915,  1717,  7114,   515,  7395,    20,  2809, 12627, 18567,\n",
      "          3374,   887, 15043, 21829,    22, 29955, 29947,    23,   915,   313,\n",
      "         29973, 29991,     7,   314,     6, 30098,   636,     4,     5, 30024,\n",
      "          1074,     0,  2167, 13592,  1791,   937, 29891,     2,   263, 29914,\n",
      "             3, 26308,    11,    10, 19012,   592,    12, 30119,     8, 30140,\n",
      "         30262,     9,  2655,   366,   817, 29902,   451,  1854,   376,   520,\n",
      "            14, 24052,   837,  1572, 24965,   280, 29872,    15, 26380,  5240,\n",
      "           861,   267, 21081,  1220,  3377,   292]], device='cuda:0') tensor([[[[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 0.,  ..., 0., 0., 0.],\n",
      "          [1., 1., 1.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [1., 0., 0.,  ..., 1., 0., 0.],\n",
      "          [1., 0., 0.,  ..., 0., 1., 0.],\n",
      "          [1., 0., 0.,  ..., 0., 1., 1.]]]], device='cuda:0') tensor([[0, 1, 2, 3, 4, 5, 4, 3, 4, 5, 6, 2, 3, 5, 5, 4, 4, 3, 1, 2, 3, 5, 2, 5,\n",
      "         1, 3, 3, 2, 6, 4, 5, 2, 5, 6, 3, 4, 5, 1, 5, 3, 6, 6, 6, 1, 2, 3, 3, 4,\n",
      "         5, 6, 5, 1, 4, 6, 3, 1, 1, 5, 2, 6, 1, 6, 5, 2, 1, 6, 3, 6, 6, 1, 2, 3,\n",
      "         4, 5, 6, 6, 2, 4, 6, 3, 1, 2, 3, 4, 6, 4, 3, 3, 5, 1, 6, 3, 4, 3, 6, 5,\n",
      "         1, 2, 6, 2, 5, 6, 3, 6, 1, 6, 2, 1, 2, 3, 6, 5, 1, 2, 1, 6, 3, 1, 1, 2,\n",
      "         6, 1, 2, 6, 5, 3, 6, 1, 3, 2, 1, 6, 1, 1, 6, 5, 3, 4, 1, 5, 6, 1, 4, 5,\n",
      "         6, 2, 5, 6, 5, 5, 1, 2, 3, 3, 2, 3, 3, 1, 2, 3, 4, 5, 2, 3, 3, 4]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "import numpy as np\n",
    "\n",
    "N_ITERATIONS = 32\n",
    "\n",
    "tree_widths = [4] #range(1, 4)\n",
    "sequential_times = []\n",
    "tree_times = []\n",
    "\n",
    "kv_cache_num_tokens = 128\n",
    "\n",
    "for tree_width in tree_widths:\n",
    "    expansion_config = (32, 2, 2, 1, 1, 1)\n",
    "\n",
    "    token_tree = _create_token_tree(\n",
    "        expansion_config=expansion_config,\n",
    "        prompt=\"The\",\n",
    "        tokenizer=tokenizer,\n",
    "        model=ssm,\n",
    "    )\n",
    "    print(token_tree)\n",
    "\n",
    "    batch_size=np.prod(expansion_config)\n",
    "    # kv_cache_sequential = _create_dummy_kv_cache(\n",
    "    #     kv_cache_num_tokens=kv_cache_num_tokens,\n",
    "    #     batch_size=batch_size,\n",
    "    #     num_attention_heads=llm.config.num_attention_heads,\n",
    "    #     hidden_size=llm.config.hidden_size,\n",
    "    #     num_layers=llm.config.num_hidden_layers\n",
    "    # )\n",
    "    \n",
    "    sequential_timer = benchmark.Timer(\n",
    "        stmt=\"time_normal(input_ids, model, kv_cache)\",\n",
    "        setup=\"from __main__ import time_normal\",\n",
    "        num_threads=torch.get_num_threads(),\n",
    "        globals={\n",
    "            'input_ids': token_tree,\n",
    "            'model': llm,\n",
    "            'kv_cache': None\n",
    "        },\n",
    "        label=\"Sequential\"\n",
    "    )\n",
    "    sequential_measurement = sequential_timer.blocked_autorange(min_run_time=1)\n",
    "    # sequential_times.append(sequential_measurement.times[-1])\n",
    "    \n",
    "    # construct inputs for tree decoding\n",
    "    # kv_cache_tree = _create_dummy_kv_cache(\n",
    "    #     kv_cache_num_tokens=kv_cache_num_tokens,\n",
    "    #     batch_size=1,\n",
    "    #     num_attention_heads=llm.config.num_attention_heads,\n",
    "    #     hidden_size=llm.config.hidden_size,\n",
    "    #     num_layers=llm.config.num_hidden_layers\n",
    "    # )\n",
    "    tree_input, tree_mask, tree_position_ids = construct_tree_model_inputs(token_tree)\n",
    "    print(tree_input, tree_mask, tree_position_ids)\n",
    "    # Required for 4D mask support in new HF\n",
    "    tree_mask = _invert_4d_attention_mask(tree_mask, kv_cache_num_tokens)\n",
    "\n",
    "    # tree_timer = benchmark.Timer(\n",
    "    #     stmt=\"time_tree(input_ids, mask, position_ids, model, kv_cache)\",\n",
    "    #     setup=\"from __main__ import time_tree\",\n",
    "    #     num_threads=torch.get_num_threads(),\n",
    "    #     globals={\n",
    "    #         'input_ids': tree_input,\n",
    "    #         'mask': tree_mask,\n",
    "    #         'position_ids': tree_position_ids,\n",
    "    #         'model': llm,\n",
    "    #         'kv_cache': None\n",
    "    #     },\n",
    "    #     label=\"Tree\"\n",
    "    # )\n",
    "    # tree_measurement = tree_timer.timeit(N_ITERATIONS)\n",
    "    # tree_times.append(tree_measurement.times[-1])\n",
    "    \n",
    "    # print_normal_profile_stats(token_tree, llm)\n",
    "    # print_tree_profile_stats(tree_input, tree_mask, tree_position_ids, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ddda5751",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.07458824850618839,\n",
       " 0.07633656449615955,\n",
       " 0.07581533119082451,\n",
       " 0.07572932541370392,\n",
       " 0.07584896497428417,\n",
       " 0.0757243912667036,\n",
       " 0.07565928436815739,\n",
       " 0.07604918070137501,\n",
       " 0.07560066692531109,\n",
       " 0.07736724242568016,\n",
       " 0.08180560730397701,\n",
       " 0.08228574879467487,\n",
       " 0.08087449707090855]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequential_measurement.times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75554c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "width = 0.35\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "x_data = tree_widths\n",
    "y_sequential = np.array(sequential_times) * 1000 # scale to ms\n",
    "plt.bar(x_data, y_sequential, label=\"Sequential\", width=width)  # Plot the first list as the y-axis values\n",
    "y_tree = np.array(tree_times) * 1000 # scale to ms\n",
    "plt.bar([pos + width for pos in x_data], y_tree, label=\"Tree\", width=width)  # Plot the second list as the y-axis values\n",
    "\n",
    "plt.xlabel(\"Tree Width\")\n",
    "plt.ylabel(\"Latency (ms)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(\"tree_vs_sequential.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cae5d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory-bound: arithmetic intensity 7.448872654272583 < 27.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import metrics\n",
    "from transformers import AutoConfig\n",
    "\n",
    "mistral_7b_config = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "metrics.identify_compute_memory_bound(\n",
    "    gpu=metrics.T4,\n",
    "    token_batch=torch.ones(1, np.sum(np.cumprod(expansion_config))),\n",
    "    dtype=torch.float32,\n",
    "    num_layers=mistral_7b_config.num_hidden_layers,\n",
    "    d_model=mistral_7b_config.hidden_size,\n",
    "    n_head=mistral_7b_config.num_attention_heads,\n",
    "    vocab_size=mistral_7b_config.vocab_size,\n",
    "    kv_cache_token_count=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
