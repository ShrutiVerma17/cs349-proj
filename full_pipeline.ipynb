{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1883387e-2829-499b-baeb-9e88d7da49fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cafe/u/shrutive/myenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/cafe/u/shrutive/myenv/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# from collections.abc import Sequence\n",
    "from typing import Sequence\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "_SSM_NAME = \"JackFram/llama-160m\"\n",
    "_LLM_NAME = 'openlm-research/open_llama_3b_v2'\n",
    "device = \"cuda\"\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "tokenizer = AutoTokenizer.from_pretrained(_SSM_NAME)\n",
    "ssm = AutoModelForCausalLM.from_pretrained(_SSM_NAME).cuda()\n",
    "llm = AutoModelForCausalLM.from_pretrained(_LLM_NAME).cuda()\n",
    "N_ITERATIONS = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d0fe080-cfbb-4cbb-8d2a-22f3acf2a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_token_tree(\n",
    "    expansion_config: Sequence[int],\n",
    "    prompt: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: AutoModelForCausalLM,\n",
    "):\n",
    "    \"\"\"Create token tree following Figure 3 in the paper.\n",
    "\n",
    "    We don't need \"real\" tokens for our experiments - just\n",
    "    random integers would work too - but might as well.\n",
    "\n",
    "    Figure 3 illustrates the <k1, k2, ...> expansion approach they\n",
    "    use to create token trees. We can use each of the top_k tokens from\n",
    "    a single model to create the same tree structure.\n",
    "\n",
    "    Args:\n",
    "        expansion_config: A sequence of integers representing how much to\n",
    "            branch at each generation step.\n",
    "        prompt: Initial prompt.\n",
    "        tokenizer: HF tokenizer.\n",
    "        model: HF generative model.\n",
    "    \"\"\"\n",
    "    assert expansion_config\n",
    "    current_tree = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "    for k in expansion_config:\n",
    "        output = model.generate(\n",
    "            current_tree,\n",
    "            max_new_tokens=1,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "        # Take the top_k tokens from the 1 generation step we've done\n",
    "        top_k = torch.topk(output.scores[-1], k=k, dim=-1).indices.reshape(-1, 1)\n",
    "        current_tree = torch.repeat_interleave(current_tree, k, dim=0)\n",
    "        # Join the top_k tokens to the current tree\n",
    "        current_tree = torch.cat((current_tree, top_k), dim=-1)\n",
    "\n",
    "    return current_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f853eaf-c00b-46b7-8424-7026c0bc8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_tree_model_inputs(sequences):\n",
    "    # input_1 = torch.unique(torch.flatten(sequences), sorted=False)\n",
    "    flat = torch.flatten(sequences).tolist()\n",
    "    unique = []\n",
    "    for tok in flat:\n",
    "        if tok not in unique:\n",
    "            unique.append(tok)\n",
    "    # input is list of unique tokens\n",
    "    input_1 = torch.tensor([unique]).to(device)\n",
    "\n",
    "    a = input_1.shape[-1]\n",
    "    mask_1 = np.zeros((a, a))\n",
    "    positions = [-1] * len(unique)\n",
    "    \n",
    "    for seq in sequences:\n",
    "        branch_progress = []\n",
    "        for (pos, tok) in enumerate(seq):\n",
    "            input_1_idx = unique.index(tok)\n",
    "            positions[input_1_idx] = pos\n",
    "            branch_progress.append(input_1_idx)\n",
    "            for idx in branch_progress:\n",
    "                mask_1[input_1_idx][idx] = 1\n",
    "    mask_1 = torch.tensor(mask_1, device=device, dtype=torch.int64)\n",
    "    mask_1 = mask_1.unsqueeze(0).unsqueeze(0).to(device).int()\n",
    "    position_ids_1 = torch.tensor([positions], device=device, dtype=torch.int64)\n",
    "    return (input_1, mask_1, position_ids_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3de2107-a040-4a06-8b3d-8b0e268a43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_normal(input, N_iterations, model: AutoModelForCausalLM):\n",
    "    total_time = 0.0\n",
    "    for i in range(N_iterations):\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            logits = model(input_ids=input).logits\n",
    "            end = time.time()\n",
    "        total_time += end - start\n",
    "    return (total_time / N_iterations)\n",
    "\n",
    "def time_tree(input, mask, position_ids, N_iterations, model: AutoModelForCausalLM):\n",
    "    total_time = 0.0\n",
    "    for i in range(N_iterations):\n",
    "        with torch.no_grad():\n",
    "            start = time.time()\n",
    "            logits = model.forward(input_ids=input, attention_mask=mask, position_ids=position_ids).logits\n",
    "            end = time.time()\n",
    "        total_time += end - start\n",
    "    return (total_time / N_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "832d137b-17d8-4813-9e1e-3d4b5d06b95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1,   450, 29871, 29896, 29900],\n",
      "        [    1,   450, 29871, 29896, 29929],\n",
      "        [    1,   450,   937,  2655,   366],\n",
      "        [    1,   450,   937,  2655,   306]], device='cuda:0')\n",
      "tensor([[    1,   450, 29871, 29896, 29900, 29929,   937,  2655,   366,   306]],\n",
      "       device='cuda:0') tensor([[[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
      "          [1, 1, 1, 1, 0, 1, 0, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 0, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 0, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 1, 0],\n",
      "          [1, 1, 0, 0, 0, 0, 1, 1, 0, 1]]]], device='cuda:0',\n",
      "       dtype=torch.int32) tensor([[0, 1, 2, 3, 4, 4, 2, 3, 4, 4]], device='cuda:0')\n",
      "Sequential Time:  0.20251182007789612\n",
      "Tree Time:  0.07197877502441406\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    token_tree = _create_token_tree(\n",
    "        expansion_config=(2, 1, 2),\n",
    "        prompt=\"The\",\n",
    "        tokenizer=tokenizer,\n",
    "        model=ssm,\n",
    "    )\n",
    "    print(token_tree)\n",
    "\n",
    "    # construct inputs for tree decoding\n",
    "    tree_input, tree_mask, tree_position_ids = construct_tree_model_inputs(token_tree)\n",
    "    print(tree_input, tree_mask, tree_position_ids)\n",
    "    \n",
    "    sequential_time = time_normal(token_tree, N_ITERATIONS, llm)\n",
    "    tree_time = time_tree(tree_input, tree_mask, tree_position_ids, N_ITERATIONS, llm)\n",
    "    print(\"Sequential Time: \", sequential_time)\n",
    "    print(\"Tree Time: \", tree_time)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185eda4d-e415-4bda-afa4-22a5c6b44908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
