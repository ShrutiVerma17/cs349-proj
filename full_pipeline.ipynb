{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1883387e-2829-499b-baeb-9e88d7da49fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# from collections.abc import Sequence\n",
    "from typing import Sequence\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "_SSM_NAME = \"JackFram/llama-160m\"\n",
    "_LLM_NAME = 'openlm-research/open_llama_3b_v2'\n",
    "device = \"cuda\"\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "tokenizer = AutoTokenizer.from_pretrained(_SSM_NAME)\n",
    "ssm = AutoModelForCausalLM.from_pretrained(_SSM_NAME).cuda()\n",
    "llm = AutoModelForCausalLM.from_pretrained(_LLM_NAME).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0fe080-cfbb-4cbb-8d2a-22f3acf2a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_token_tree(\n",
    "    expansion_config: Sequence[int],\n",
    "    prompt: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: AutoModelForCausalLM,\n",
    "    has_kv_cache: bool = False,\n",
    "):\n",
    "    \"\"\"Create token tree following Figure 3 in the paper.\n",
    "\n",
    "    We don't need \"real\" tokens for our experiments - just\n",
    "    random integers would work too - but might as well.\n",
    "\n",
    "    Figure 3 illustrates the <k1, k2, ...> expansion approach they\n",
    "    use to create token trees. We can use each of the top_k tokens from\n",
    "    a single model to create the same tree structure.\n",
    "\n",
    "    Args:\n",
    "        expansion_config: A sequence of integers representing how much to\n",
    "            branch at each generation step.\n",
    "        prompt: Initial prompt.\n",
    "        tokenizer: HF tokenizer.\n",
    "        model: HF generative model.\n",
    "        has_kv_cache: If there are preceding tokens this is generating after, in which\n",
    "            case we exclude the begin-of-sequence token.\n",
    "    \"\"\"\n",
    "    assert expansion_config\n",
    "    current_tree = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    if has_kv_cache:\n",
    "        assert tokenizer.add_bos_token\n",
    "        current_tree = current_tree[:, 1:]\n",
    "    for k in expansion_config:\n",
    "        output = model.generate(\n",
    "            current_tree,\n",
    "            max_new_tokens=1,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "        # Take the top_k tokens from the 1 generation step we've done\n",
    "        top_k = torch.topk(output.scores[-1], k=k, dim=-1).indices.reshape(-1, 1)\n",
    "        current_tree = torch.repeat_interleave(current_tree, k, dim=0)\n",
    "        # Join the top_k tokens to the current tree\n",
    "        current_tree = torch.cat((current_tree, top_k), dim=-1)\n",
    "\n",
    "    return current_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f853eaf-c00b-46b7-8424-7026c0bc8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _invert_4d_attention_mask(attention_mask: torch.Tensor, kv_cache_num_tokens: int=0) -> torch.Tensor:\n",
    "    \"\"\"For 4D masks, new HF requires us to invert the mask so it doesn't modify it at all.\"\"\"\n",
    "    # The attention mask must have last 2 dims shape [current seq len, KV cache size + current seq len]\n",
    "    # So we prepend a tensor of 1s to allow attending to the full KV cache\n",
    "    assert attention_mask.dim() == 4\n",
    "    if kv_cache_num_tokens > 0:\n",
    "        attention_mask = torch.cat(\n",
    "            (\n",
    "                torch.ones(\n",
    "                    attention_mask.shape[0],\n",
    "                    attention_mask.shape[1],\n",
    "                    attention_mask.shape[2],\n",
    "                    kv_cache_num_tokens,\n",
    "                ).to(device),\n",
    "                attention_mask,\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "    # Invert the mask: 0s to -inf and 1s to 0 (0 means attention allowed)\n",
    "    min_dtype = torch.finfo(torch.float32).min\n",
    "    attention_mask.masked_fill_(attention_mask == 0, min_dtype)\n",
    "    attention_mask.masked_fill_(attention_mask == 1, 0.0)\n",
    "    return attention_mask\n",
    "\n",
    "def construct_tree_model_inputs(sequences):\n",
    "    # input_1 = torch.unique(torch.flatten(sequences), sorted=False)\n",
    "    flat = torch.flatten(sequences).tolist()\n",
    "    unique = []\n",
    "    for tok in flat:\n",
    "        if tok not in unique:\n",
    "            unique.append(tok)\n",
    "    # input is list of unique tokens\n",
    "    input_1 = torch.tensor([unique]).to(device)\n",
    "\n",
    "    a = input_1.shape[-1]\n",
    "    mask_1 = np.zeros((a, a), dtype=np.float32)\n",
    "    positions = [-1] * len(unique)\n",
    "    \n",
    "    for seq in sequences:\n",
    "        branch_progress = []\n",
    "        for (pos, tok) in enumerate(seq):\n",
    "            input_1_idx = unique.index(tok)\n",
    "            positions[input_1_idx] = pos\n",
    "            branch_progress.append(input_1_idx)\n",
    "            for idx in branch_progress:\n",
    "                mask_1[input_1_idx][idx] = 1\n",
    "    mask_1 = torch.tensor(mask_1, device=device)\n",
    "    mask_1 = mask_1.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    position_ids_1 = torch.tensor([positions], device=device, dtype=torch.int64)\n",
    "    return (input_1, mask_1, position_ids_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5614283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_dummy_kv_cache(\n",
    "    kv_cache_num_tokens: int,\n",
    "    batch_size: int,\n",
    "    num_attention_heads: int,\n",
    "    hidden_size: int,\n",
    "    num_layers: int,\n",
    "):\n",
    "    k = torch.rand(\n",
    "        batch_size,\n",
    "        num_attention_heads,\n",
    "        kv_cache_num_tokens,\n",
    "        hidden_size // num_attention_heads,\n",
    "    ).to(device)\n",
    "    v = torch.rand(\n",
    "        batch_size,\n",
    "        num_attention_heads,\n",
    "        kv_cache_num_tokens,\n",
    "        hidden_size // num_attention_heads,\n",
    "    ).to(device)\n",
    "    return tuple((k, v) for _ in range(num_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de2107-a040-4a06-8b3d-8b0e268a43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_normal(input_ids, model: AutoModelForCausalLM, kv_cache=None):\n",
    "    with torch.inference_mode():\n",
    "        model(input_ids=input_ids, past_key_values=kv_cache, use_cache=kv_cache is not None)\n",
    "\n",
    "def time_tree(input_ids, mask, position_ids, model: AutoModelForCausalLM, kv_cache=None):\n",
    "    with torch.inference_mode():\n",
    "        model(input_ids=input_ids, attention_mask=mask, position_ids=position_ids, past_key_values=kv_cache, use_cache=kv_cache is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e0930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, ProfilerActivity, schedule\n",
    "\n",
    "# Guide: https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n",
    "\n",
    "_N_ITERATIONS = 10\n",
    "_WAIT_STEPS = 1\n",
    "_WARMUP_STEPS = 1\n",
    "schedule_params = {\n",
    "    'wait': _WAIT_STEPS,\n",
    "    'warmup': _WARMUP_STEPS,\n",
    "    'active': _N_ITERATIONS - _WAIT_STEPS - _WARMUP_STEPS,\n",
    "}\n",
    "profiler_kwargs = {\n",
    "    'activities': [ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    'profile_memory': True,\n",
    "    'schedule': schedule(**schedule_params),\n",
    "    'record_shapes': True\n",
    "}\n",
    "\n",
    "def print_normal_profile_stats(input, model):\n",
    "    with torch.inference_mode(), profile(**profiler_kwargs) as prof:\n",
    "        for _ in range(_N_ITERATIONS):\n",
    "            model(input_ids=input)\n",
    "            prof.step()\n",
    "    print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n",
    "\n",
    "def print_tree_profile_stats(input, mask, position_ids, model):\n",
    "    with torch.inference_mode(), profile(**profiler_kwargs) as prof:\n",
    "        for _ in range(_N_ITERATIONS):\n",
    "            model(input_ids=input, attention_mask=mask, position_ids=position_ids)\n",
    "            prof.step()\n",
    "    print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68d28f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure max memory allocated\n",
    "import gc\n",
    "\n",
    "def reset_memory():\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_max_memory_allocated()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "def end_memory_collection():\n",
    "    torch.cuda.synchronize()\n",
    "    max_mem_gb = torch.cuda.max_memory_allocated() / 1024**3\n",
    "    return max_mem_gb\n",
    "\n",
    "token_tree = _create_token_tree([2, 2, 2], \"The\", tokenizer, ssm)\n",
    "reset_memory()\n",
    "time_normal(token_tree, llm, kv_cache=None)\n",
    "seq_max_mem_gb = end_memory_collection()\n",
    "\n",
    "tree_input, tree_mask, tree_position_ids = construct_tree_model_inputs(token_tree)\n",
    "tree_mask = _invert_4d_attention_mask(tree_mask)\n",
    "reset_memory()\n",
    "time_tree(input_ids=tree_input, mask=tree_mask, position_ids=tree_position_ids, model=llm, kv_cache=None)\n",
    "tree_max_mem_gb = end_memory_collection()\n",
    "\n",
    "print(f\"Normal: {seq_max_mem_gb:.2f} GB\")\n",
    "print(f\"Tree: {tree_max_mem_gb:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d137b-17d8-4813-9e1e-3d4b5d06b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.benchmark as benchmark\n",
    "import numpy as np\n",
    "\n",
    "N_ITERATIONS = 32\n",
    "\n",
    "tree_widths = range(1, 4)\n",
    "sequential_times = []\n",
    "tree_times = []\n",
    "\n",
    "kv_cache_num_tokens = 128\n",
    "\n",
    "for tree_width in tree_widths:\n",
    "    expansion_config = (tree_width, 1, 1, 1, 1)\n",
    "\n",
    "    token_tree = _create_token_tree(\n",
    "        expansion_config=expansion_config,\n",
    "        prompt=\"The\",\n",
    "        tokenizer=tokenizer,\n",
    "        model=ssm,\n",
    "    )\n",
    "    print(token_tree)\n",
    "\n",
    "    batch_size=np.prod(expansion_config)\n",
    "    kv_cache_sequential = _create_dummy_kv_cache(\n",
    "        kv_cache_num_tokens=kv_cache_num_tokens,\n",
    "        batch_size=batch_size,\n",
    "        num_attention_heads=llm.config.num_attention_heads,\n",
    "        hidden_size=llm.config.hidden_size,\n",
    "        num_layers=llm.config.num_hidden_layers\n",
    "    )\n",
    "    \n",
    "    sequential_timer = benchmark.Timer(\n",
    "        stmt=\"time_normal(input_ids, model, kv_cache)\",\n",
    "        setup=\"from __main__ import time_normal\",\n",
    "        num_threads=torch.get_num_threads(),\n",
    "        globals={\n",
    "            'input_ids': token_tree,\n",
    "            'model': llm,\n",
    "            'kv_cache': kv_cache_sequential\n",
    "        },\n",
    "        label=\"Sequential\"\n",
    "    )\n",
    "    sequential_measurement = sequential_timer.timeit(N_ITERATIONS)\n",
    "    sequential_times.append(sequential_measurement.times[-1])\n",
    "    \n",
    "    # construct inputs for tree decoding\n",
    "    kv_cache_tree = _create_dummy_kv_cache(\n",
    "        kv_cache_num_tokens=kv_cache_num_tokens,\n",
    "        batch_size=1,\n",
    "        num_attention_heads=llm.config.num_attention_heads,\n",
    "        hidden_size=llm.config.hidden_size,\n",
    "        num_layers=llm.config.num_hidden_layers\n",
    "    )\n",
    "    tree_input, tree_mask, tree_position_ids = construct_tree_model_inputs(token_tree)\n",
    "    print(tree_input, tree_mask, tree_position_ids)\n",
    "    # Required for 4D mask support in new HF\n",
    "    tree_mask = _invert_4d_attention_mask(tree_mask, kv_cache_num_tokens)\n",
    "\n",
    "    tree_timer = benchmark.Timer(\n",
    "        stmt=\"time_tree(input_ids, mask, position_ids, model, kv_cache)\",\n",
    "        setup=\"from __main__ import time_tree\",\n",
    "        num_threads=torch.get_num_threads(),\n",
    "        globals={\n",
    "            'input_ids': tree_input,\n",
    "            'mask': tree_mask,\n",
    "            'position_ids': tree_position_ids,\n",
    "            'model': llm,\n",
    "            'kv_cache': kv_cache_tree\n",
    "        },\n",
    "        label=\"Tree\"\n",
    "    )\n",
    "    tree_measurement = tree_timer.timeit(N_ITERATIONS)\n",
    "    tree_times.append(tree_measurement.times[-1])\n",
    "    \n",
    "    # print_normal_profile_stats(token_tree, llm)\n",
    "    # print_tree_profile_stats(tree_input, tree_mask, tree_position_ids, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75554c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "width = 0.35\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "x_data = tree_widths\n",
    "y_sequential = np.array(sequential_times) * 1000 # scale to ms\n",
    "plt.bar(x_data, y_sequential, label=\"Sequential\", width=width)  # Plot the first list as the y-axis values\n",
    "y_tree = np.array(tree_times) * 1000 # scale to ms\n",
    "plt.bar([pos + width for pos in x_data], y_tree, label=\"Tree\", width=width)  # Plot the second list as the y-axis values\n",
    "\n",
    "plt.xlabel(\"Tree Width\")\n",
    "plt.ylabel(\"Latency (ms)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(\"tree_vs_sequential.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cae5d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory-bound: arithmetic intensity 7.448872654272583 < 27.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nlp/scr/ananthag/miniconda3/envs/spec/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import metrics\n",
    "from transformers import AutoConfig\n",
    "\n",
    "mistral_7b_config = AutoConfig.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "\n",
    "metrics.identify_compute_memory_bound(\n",
    "    gpu=metrics.T4,\n",
    "    token_batch=torch.ones(1, np.sum(np.cumprod(expansion_config))),\n",
    "    dtype=torch.float32,\n",
    "    num_layers=mistral_7b_config.num_hidden_layers,\n",
    "    d_model=mistral_7b_config.hidden_size,\n",
    "    n_head=mistral_7b_config.num_attention_heads,\n",
    "    vocab_size=mistral_7b_config.vocab_size,\n",
    "    kv_cache_token_count=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
