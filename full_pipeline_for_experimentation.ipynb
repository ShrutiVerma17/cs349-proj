{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69cd485e-f284-47d6-8cd1-c87df59a9849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution -illow (/cafe/u/shrutive/myenv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/cafe/u/shrutive/myenv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in ./myenv/lib/python3.8/site-packages (4.38.1)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./myenv/lib/python3.8/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: numpy>=1.17 in ./myenv/lib/python3.8/site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./myenv/lib/python3.8/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in ./myenv/lib/python3.8/site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./myenv/lib/python3.8/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in ./myenv/lib/python3.8/site-packages (from transformers) (0.23.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./myenv/lib/python3.8/site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: filelock in ./myenv/lib/python3.8/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in ./myenv/lib/python3.8/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./myenv/lib/python3.8/site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./myenv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./myenv/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./myenv/lib/python3.8/site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./myenv/lib/python3.8/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./myenv/lib/python3.8/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./myenv/lib/python3.8/site-packages (from requests->transformers) (1.26.13)\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -illow (/cafe/u/shrutive/myenv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: transformers\n",
      "  Attempting uninstall: transformers\n",
      "\u001b[33m    WARNING: Ignoring invalid distribution -illow (/cafe/u/shrutive/myenv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: transformers 4.38.1\n",
      "    Uninstalling transformers-4.38.1:\n",
      "      Successfully uninstalled transformers-4.38.1\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -illow (/cafe/u/shrutive/myenv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed transformers-4.41.2\n",
      "\u001b[33mWARNING: Ignoring invalid distribution -illow (/cafe/u/shrutive/myenv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/cafe/u/shrutive/myenv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -illow (/cafe/u/shrutive/myenv/lib/python3.8/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.3; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/cafe/u/shrutive/myenv/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2eeba3db-5540-4a9d-b621-575581f4d9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cafe/u/shrutive/myenv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.41.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1883387e-2829-499b-baeb-9e88d7da49fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cafe/u/shrutive/myenv/lib/python3.8/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# from collections.abc import Sequence\n",
    "from typing import Sequence\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "_SSM_NAME = \"JackFram/llama-160m\"\n",
    "_LLM_NAME = 'openlm-research/open_llama_3b_v2'\n",
    "device = \"cuda\"\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "tokenizer = AutoTokenizer.from_pretrained(_SSM_NAME)\n",
    "ssm = AutoModelForCausalLM.from_pretrained(_SSM_NAME, device_map='auto')\n",
    "llm = AutoModelForCausalLM.from_pretrained(_LLM_NAME, device_map='auto')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d0fe080-cfbb-4cbb-8d2a-22f3acf2a232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_token_tree(\n",
    "    expansion_config: Sequence[int],\n",
    "    prompt: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: AutoModelForCausalLM,\n",
    "    has_kv_cache: bool = False,\n",
    "):\n",
    "    \"\"\"Create token tree following Figure 3 in the paper.\n",
    "\n",
    "    We don't need \"real\" tokens for our experiments - just\n",
    "    random integers would work too - but might as well.\n",
    "\n",
    "    Figure 3 illustrates the <k1, k2, ...> expansion approach they\n",
    "    use to create token trees. We can use each of the top_k tokens from\n",
    "    a single model to create the same tree structure.\n",
    "\n",
    "    Args:\n",
    "        expansion_config: A sequence of integers representing how much to\n",
    "            branch at each generation step.\n",
    "        prompt: Initial prompt.\n",
    "        tokenizer: HF tokenizer.\n",
    "        model: HF generative model.\n",
    "    \"\"\"\n",
    "    assert expansion_config\n",
    "    current_tree = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "    if has_kv_cache:\n",
    "        assert tokenizer.add_bos_token\n",
    "        current_tree = current_tree[:, 1:]\n",
    "    for k in expansion_config:\n",
    "        output = model.generate(\n",
    "            current_tree,\n",
    "            max_new_tokens=1,\n",
    "            return_dict_in_generate=True,\n",
    "            output_scores=True,\n",
    "        )\n",
    "        # Take the top_k tokens from the 1 generation step we've done\n",
    "        top_k = torch.topk(output.scores[-1], k=k, dim=-1).indices.reshape(-1, 1)\n",
    "        current_tree = torch.repeat_interleave(current_tree, k, dim=0)\n",
    "        # Join the top_k tokens to the current tree\n",
    "        current_tree = torch.cat((current_tree, top_k), dim=-1)\n",
    "\n",
    "    return current_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f853eaf-c00b-46b7-8424-7026c0bc8cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _invert_4d_attention_mask(attention_mask: torch.Tensor, kv_cache_num_tokens: int=0) -> torch.Tensor:\n",
    "    \"\"\"For 4D masks, new HF requires us to invert the mask so it doesn't modify it at all.\"\"\"\n",
    "    # The attention mask must have last 2 dims shape [current seq len, KV cache size + current seq len]\n",
    "    # So we prepend a tensor of 1s to allow attending to the full KV cache\n",
    "    assert attention_mask.dim() == 4\n",
    "    if kv_cache_num_tokens > 0:\n",
    "        attention_mask = torch.cat(\n",
    "            (\n",
    "                torch.ones(\n",
    "                    attention_mask.shape[0],\n",
    "                    attention_mask.shape[1],\n",
    "                    attention_mask.shape[2],\n",
    "                    kv_cache_num_tokens,\n",
    "                ).to(device),\n",
    "                attention_mask,\n",
    "            ),\n",
    "            dim=-1,\n",
    "        )\n",
    "    # Invert the mask: 0s to -inf and 1s to 0 (0 means attention allowed)\n",
    "    min_dtype = torch.finfo(torch.float32).min\n",
    "    min_dtype = -1e+30 if attention_mask.dtype == torch.float32 else -1e+4\n",
    "    attention_mask.masked_fill_(attention_mask == 0.0, min_dtype)\n",
    "    attention_mask.masked_fill_(attention_mask == 1, 0.0)\n",
    "    return attention_mask\n",
    "\n",
    "def construct_tree_model_inputs(sequences):\n",
    "    # input_1 = torch.unique(torch.flatten(sequences), sorted=False)\n",
    "    flat = torch.flatten(sequences).tolist()\n",
    "    unique = []\n",
    "    for tok in flat:\n",
    "        if tok not in unique:\n",
    "            unique.append(tok)\n",
    "    # input is list of unique tokens\n",
    "    input_1 = torch.tensor([unique]).to(device)\n",
    "\n",
    "    a = input_1.shape[-1]\n",
    "    mask_1 = np.zeros((a, a))\n",
    "    positions = [-1] * len(unique)\n",
    "    \n",
    "    for seq in sequences:\n",
    "        branch_progress = []\n",
    "        for (pos, tok) in enumerate(seq):\n",
    "            input_1_idx = unique.index(tok)\n",
    "            positions[input_1_idx] = pos\n",
    "            branch_progress.append(input_1_idx)\n",
    "            for idx in branch_progress:\n",
    "                mask_1[input_1_idx][idx] = 1\n",
    "    mask_1 = torch.tensor(mask_1, device=device, dtype=torch.int64)\n",
    "    mask_1 = mask_1.unsqueeze(0).unsqueeze(0).to(device).int()\n",
    "    position_ids_1 = torch.tensor([positions], device=device, dtype=torch.int64)\n",
    "    return (input_1, mask_1, position_ids_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a79bf549-db26-40e8-aeb3-d50b8babd7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_dummy_kv_cache(\n",
    "    kv_cache_num_tokens: int,\n",
    "    batch_size: int,\n",
    "    num_attention_heads: int,\n",
    "    hidden_size: int,\n",
    "    num_layers: int,\n",
    "):\n",
    "    k = torch.rand(\n",
    "        batch_size,\n",
    "        num_attention_heads,\n",
    "        kv_cache_num_tokens,\n",
    "        hidden_size // num_attention_heads,\n",
    "    ).to(device)\n",
    "    v = torch.rand(\n",
    "        batch_size,\n",
    "        num_attention_heads,\n",
    "        kv_cache_num_tokens,\n",
    "        hidden_size // num_attention_heads,\n",
    "    ).to(device)\n",
    "    return tuple((k, v) for _ in range(num_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3de2107-a040-4a06-8b3d-8b0e268a43ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_normal(input_ids, model: AutoModelForCausalLM):\n",
    "    with torch.inference_mode():\n",
    "        model(input_ids=input_ids)\n",
    "\n",
    "def time_tree(input_ids, mask, position_ids, model: AutoModelForCausalLM):\n",
    "    with torch.inference_mode():\n",
    "        model(input_ids=input_ids, attention_mask=mask, position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9e0930e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.profiler import profile, ProfilerActivity, schedule\n",
    "\n",
    "# Guide: https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n",
    "\n",
    "_N_ITERATIONS = 10\n",
    "_WAIT_STEPS = 1\n",
    "_WARMUP_STEPS = 1\n",
    "schedule_params = {\n",
    "    'wait': _WAIT_STEPS,\n",
    "    'warmup': _WARMUP_STEPS,\n",
    "    'active': _N_ITERATIONS - _WAIT_STEPS - _WARMUP_STEPS,\n",
    "}\n",
    "profiler_kwargs = {\n",
    "    'activities': [ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
    "    'profile_memory': True,\n",
    "    'schedule': schedule(**schedule_params),\n",
    "    'record_shapes': True\n",
    "}\n",
    "\n",
    "def print_normal_profile_stats(input, model):\n",
    "    with torch.inference_mode(), profile(**profiler_kwargs) as prof:\n",
    "        for _ in range(_N_ITERATIONS):\n",
    "            model(input_ids=input, past_key_values = torch.tensor([3, 5, 7]), use_cache = True)\n",
    "            prof.step()\n",
    "    print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))\n",
    "\n",
    "def print_tree_profile_stats(input, mask, position_ids, model):\n",
    "    with torch.inference_mode(), profile(**profiler_kwargs) as prof:\n",
    "        for _ in range(_N_ITERATIONS):\n",
    "            model(input_ids=input, attention_mask=mask, position_ids=position_ids, past_key_values = torch.tensor([1, 2, 3]), use_cache = True)\n",
    "            prof.step()\n",
    "    print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b6e469e-fd42-41a4-ac31-8d88fe676a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_factors(n):    \n",
    "    return [[i, n//i] for i in range(1, int(n**0.5) + 1) if n % i == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ae87ed8-3213-4b5f-87be-4337209f774d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools \n",
    "def generate_expansion_configs(config_length, min_expansion, max_expansion):\n",
    "    values = [i for i in range(min_expansion, max_expansion)]\n",
    "    result = list(itertools.product(values, repeat=config_length))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71a246d0-100f-4da1-88d1-940b34221e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------\n",
      "(2, 2, 2), 0\n",
      "Sequential Time:  0.07214655948337168\n",
      "Tree Time:  0.03981226298492402\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 2), 128\n",
      "Sequential Time:  0.07203928218223155\n",
      "Tree Time:  0.039799969643354416\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 2), 256\n",
      "Sequential Time:  0.07204580714460462\n",
      "Tree Time:  0.03979276749305427\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 2), 512\n",
      "Sequential Time:  0.07203923910856247\n",
      "Tree Time:  0.03983791545033455\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 2), 1024\n",
      "Sequential Time:  0.0720571915153414\n",
      "Tree Time:  0.03987861319910735\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 2), 2048\n",
      "Sequential Time:  0.07208873436320573\n",
      "Tree Time:  0.03985221148468554\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 3), 0\n",
      "Sequential Time:  0.07398299698252231\n",
      "Tree Time:  0.04084786877501756\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 3), 128\n",
      "Sequential Time:  0.07402023312170058\n",
      "Tree Time:  0.04078805272001773\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 3), 256\n",
      "Sequential Time:  0.07401675870642066\n",
      "Tree Time:  0.04084043705370277\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 3), 512\n",
      "Sequential Time:  0.07404990692157298\n",
      "Tree Time:  0.04083968431223184\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 3), 1024\n",
      "Sequential Time:  0.07405067188665271\n",
      "Tree Time:  0.040878327912651\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 3), 2048\n",
      "Sequential Time:  0.07406679913401604\n",
      "Tree Time:  0.04089690139517188\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 4), 0\n",
      "Sequential Time:  0.08576606574933976\n",
      "Tree Time:  0.04115101438947022\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 4), 128\n",
      "Sequential Time:  0.0858865964692086\n",
      "Tree Time:  0.041095578228123486\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 4), 256\n",
      "Sequential Time:  0.0859351777471602\n",
      "Tree Time:  0.041108992183580995\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 4), 512\n",
      "Sequential Time:  0.08598539978265762\n",
      "Tree Time:  0.041120053734630346\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 4), 1024\n",
      "Sequential Time:  0.086022236966528\n",
      "Tree Time:  0.041138626518659294\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 4), 2048\n",
      "Sequential Time:  0.08612404693849385\n",
      "Tree Time:  0.041151736746542156\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 5), 0\n",
      "Sequential Time:  0.08784728520549834\n",
      "Tree Time:  0.04137892706785351\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 5), 128\n",
      "Sequential Time:  0.08790166233666241\n",
      "Tree Time:  0.04130783025175333\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 5), 256\n",
      "Sequential Time:  0.08796700206585228\n",
      "Tree Time:  0.041317996801808476\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 5), 512\n",
      "Sequential Time:  0.08801256772130728\n",
      "Tree Time:  0.041335525340400636\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 5), 1024\n",
      "Sequential Time:  0.08809089555870742\n",
      "Tree Time:  0.04136690043378621\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 5), 2048\n",
      "Sequential Time:  0.08819499448873103\n",
      "Tree Time:  0.041375646251253784\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 6), 0\n",
      "Sequential Time:  0.09352358512114733\n",
      "Tree Time:  0.0415616569807753\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 6), 128\n",
      "Sequential Time:  0.0935669724131003\n",
      "Tree Time:  0.041538836201652884\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 6), 256\n",
      "Sequential Time:  0.0936439975630492\n",
      "Tree Time:  0.04153802595101297\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 6), 512\n",
      "Sequential Time:  0.09364681760780513\n",
      "Tree Time:  0.04162980325054377\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 6), 1024\n",
      "Sequential Time:  0.09374973736703396\n",
      "Tree Time:  0.041714496561326087\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 6), 2048\n",
      "Sequential Time:  0.0937677341280505\n",
      "Tree Time:  0.04194501449819654\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 7), 0\n",
      "Sequential Time:  0.14114108425565064\n",
      "Tree Time:  0.04233406903222203\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 7), 128\n",
      "Sequential Time:  0.14261658990290016\n",
      "Tree Time:  0.04247051547281444\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 7), 256\n",
      "Sequential Time:  0.14393169502727687\n",
      "Tree Time:  0.04262707324232906\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 7), 512\n",
      "Sequential Time:  0.14482736284844577\n",
      "Tree Time:  0.04283530777320266\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 7), 1024\n",
      "Sequential Time:  0.14521448919549584\n",
      "Tree Time:  0.04300523817073554\n",
      "-----------\n",
      "-----------\n",
      "(2, 2, 7), 2048\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 700.00 MiB (GPU 0; 23.65 GiB total capacity; 21.64 GiB already allocated; 585.44 MiB free; 22.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 35\u001b[0m\n\u001b[1;32m     27\u001b[0m token_tree \u001b[38;5;241m=\u001b[39m _create_token_tree(\n\u001b[1;32m     28\u001b[0m         expansion_config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m     29\u001b[0m         prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     30\u001b[0m         tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     31\u001b[0m         model\u001b[38;5;241m=\u001b[39mssm,\n\u001b[1;32m     32\u001b[0m     )\n\u001b[1;32m     34\u001b[0m batch_size\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mprod(config)\n\u001b[0;32m---> 35\u001b[0m kv_cache_sequential \u001b[38;5;241m=\u001b[39m \u001b[43m_create_dummy_kv_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkv_cache_num_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkv_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_attention_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_hidden_layers\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m sequential_timer \u001b[38;5;241m=\u001b[39m benchmark\u001b[38;5;241m.\u001b[39mTimer(\n\u001b[1;32m     44\u001b[0m     stmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_normal(input_ids, model)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     45\u001b[0m     setup\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom __main__ import time_normal\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     52\u001b[0m     label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequential\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     53\u001b[0m )\n\u001b[1;32m     54\u001b[0m sequential_measurement \u001b[38;5;241m=\u001b[39m sequential_timer\u001b[38;5;241m.\u001b[39mtimeit(N_ITERATIONS)\n",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m, in \u001b[0;36m_create_dummy_kv_cache\u001b[0;34m(kv_cache_num_tokens, batch_size, num_attention_heads, hidden_size, num_layers)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_dummy_kv_cache\u001b[39m(\n\u001b[1;32m      2\u001b[0m     kv_cache_num_tokens: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m      3\u001b[0m     batch_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m     num_layers: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m      7\u001b[0m ):\n\u001b[1;32m      8\u001b[0m     k \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(\n\u001b[1;32m      9\u001b[0m         batch_size,\n\u001b[1;32m     10\u001b[0m         num_attention_heads,\n\u001b[1;32m     11\u001b[0m         kv_cache_num_tokens,\n\u001b[1;32m     12\u001b[0m         hidden_size \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_attention_heads,\n\u001b[1;32m     13\u001b[0m     )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 14\u001b[0m     v \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkv_cache_num_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_attention_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m((k, v) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers))\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 700.00 MiB (GPU 0; 23.65 GiB total capacity; 21.64 GiB already allocated; 585.44 MiB free; 22.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# NEW: \n",
    "import torch.utils.benchmark as benchmark\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "N_ITERATIONS = 32\n",
    "\n",
    "#tree_lengths = range(1, 10)\n",
    "# B = range(2, 10)\n",
    "# sequential_times = []\n",
    "# tree_times = []\n",
    "\n",
    "expansion_configs = generate_expansion_configs(3, 2, 8) # first arg = length of config, second arg = min val in config, third = max\n",
    "kv_sizes = [0, 128, 256, 512, 1024, 2048]\n",
    "# expansion_configs = [(7, 7, 7)]\n",
    "# kv_sizes = [1024]\n",
    "\n",
    "# past_key_values, need tuple of two tensors of shape (batch_size, num_heads, sequence_length, embed_size_per_head))\n",
    "sequential_times = {}\n",
    "tree_times = {}\n",
    "\n",
    "for config in expansion_configs: \n",
    "    for kv_size in kv_sizes:\n",
    "        print(\"-----------\")\n",
    "        overall_conf = str(config) + \", \" + str(kv_size)\n",
    "        print(overall_conf)\n",
    "        token_tree = _create_token_tree(\n",
    "                expansion_config=config,\n",
    "                prompt=\"The\",\n",
    "                tokenizer=tokenizer,\n",
    "                model=ssm,\n",
    "            )\n",
    "\n",
    "        batch_size=np.prod(config)\n",
    "        kv_cache_sequential = _create_dummy_kv_cache(\n",
    "            kv_cache_num_tokens=kv_size,\n",
    "            batch_size=batch_size,\n",
    "            num_attention_heads=llm.config.num_attention_heads,\n",
    "            hidden_size=llm.config.hidden_size,\n",
    "            num_layers=llm.config.num_hidden_layers\n",
    "        )\n",
    "        \n",
    "        sequential_timer = benchmark.Timer(\n",
    "            stmt=\"time_normal(input_ids, model)\",\n",
    "            setup=\"from __main__ import time_normal\",\n",
    "            num_threads=torch.get_num_threads(),\n",
    "            globals={\n",
    "                'input_ids': token_tree,\n",
    "                'model': llm,\n",
    "                'kv_cache': kv_cache_sequential\n",
    "            },\n",
    "            label=\"Sequential\"\n",
    "        )\n",
    "        sequential_measurement = sequential_timer.timeit(N_ITERATIONS)\n",
    "        seq_time = sequential_measurement.times[-1]\n",
    "        sequential_times[overall_conf] = seq_time\n",
    "        print(\"Sequential Time: \", seq_time)\n",
    "        \n",
    "        # construct inputs for tree decoding\n",
    "        kv_cache_tree = _create_dummy_kv_cache(\n",
    "            kv_cache_num_tokens=kv_size,\n",
    "            batch_size=1,\n",
    "            num_attention_heads=llm.config.num_attention_heads,\n",
    "            hidden_size=llm.config.hidden_size,\n",
    "            num_layers=llm.config.num_hidden_layers\n",
    "        )\n",
    "        tree_input, tree_mask, tree_position_ids = construct_tree_model_inputs(token_tree)\n",
    "        # Required for 4D mask support in new HF\n",
    "        tree_mask = _invert_4d_attention_mask(tree_mask, kv_size)\n",
    "        tree_timer = benchmark.Timer(\n",
    "            stmt=\"time_tree(input_ids, mask, position_ids, model)\",\n",
    "            setup=\"from __main__ import time_tree\",\n",
    "            num_threads=torch.get_num_threads(),\n",
    "            globals={\n",
    "                'input_ids': tree_input,\n",
    "                'mask': tree_mask,\n",
    "                'position_ids': tree_position_ids,\n",
    "                'model': llm,\n",
    "                'kv_cache': kv_cache_tree\n",
    "            },\n",
    "            label=\"Tree\"\n",
    "        )\n",
    "        token_tree = token_tree.cpu()\n",
    "        tree_input = tree_input.cpu()\n",
    "        tree_mask = tree_mask.cpu()\n",
    "        tree_position_ids = tree_position_ids.cpu()\n",
    "        tree_measurement = tree_timer.timeit(N_ITERATIONS)\n",
    "        tree_time = tree_measurement.times[-1]\n",
    "        tree_times[overall_conf] = tree_time\n",
    "        print(\"Tree Time: \", tree_time)\n",
    "        print(\"-----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa74f35-530d-4d80-9053-1ee73af18e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_times[(2, 2, 3)]\n",
    "tree_times[(2, 2, 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832d137b-17d8-4813-9e1e-3d4b5d06b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD\n",
    "import torch.utils.benchmark as benchmark\n",
    "\n",
    "N_ITERATIONS = 32\n",
    "\n",
    "tree_lengths = range(1, 10)\n",
    "sequential_times = []\n",
    "tree_times = []\n",
    "\n",
    "for tree_length in tree_lengths:\n",
    "    config = [2 for i in range(tree_length + 1)]\n",
    "    token_tree = _create_token_tree(\n",
    "        expansion_config=config,\n",
    "        prompt=\"The\",\n",
    "        tokenizer=tokenizer,\n",
    "        model=ssm,\n",
    "    )\n",
    "    print(token_tree)\n",
    "    \n",
    "    sequential_timer = benchmark.Timer(\n",
    "        stmt=\"time_normal(input_ids, model)\",\n",
    "        setup=\"from __main__ import time_normal\",\n",
    "        num_threads=torch.get_num_threads(),\n",
    "        globals={\n",
    "            'input_ids': token_tree,\n",
    "            'model': llm\n",
    "        },\n",
    "        label=\"Sequential\"\n",
    "    )\n",
    "    sequential_measurement = sequential_timer.timeit(N_ITERATIONS)\n",
    "    sequential_times.append(sequential_measurement.times[-1])\n",
    "    \n",
    "    # construct inputs for tree decoding\n",
    "    tree_input, tree_mask, tree_position_ids = construct_tree_model_inputs(token_tree)\n",
    "    print(tree_input, tree_mask, tree_position_ids)\n",
    "\n",
    "    tree_timer = benchmark.Timer(\n",
    "        stmt=\"time_tree(input_ids, mask, position_ids, model)\",\n",
    "        setup=\"from __main__ import time_tree\",\n",
    "        num_threads=torch.get_num_threads(),\n",
    "        globals={\n",
    "            'input_ids': tree_input,\n",
    "            'mask': tree_mask,\n",
    "            'position_ids': tree_position_ids,\n",
    "            'model': llm\n",
    "        },\n",
    "        label=\"Tree\"\n",
    "    )\n",
    "    tree_measurement = tree_timer.timeit(N_ITERATIONS)\n",
    "    tree_times.append(tree_measurement.times[-1])\n",
    "    \n",
    "    # print_normal_profile_stats(token_tree, llm)\n",
    "    # print_tree_profile_stats(tree_input, tree_mask, tree_position_ids, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75554c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "width = 0.35\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "x_data = tree_widths\n",
    "y_sequential = np.array(sequential_times) * 1000 # scale to ms\n",
    "plt.bar(x_data, y_sequential, label=\"Sequential\", width=width)  # Plot the first list as the y-axis values\n",
    "y_tree = np.array(tree_times) * 1000 # scale to ms\n",
    "plt.bar([pos + width for pos in x_data], y_tree, label=\"Tree\", width=width)  # Plot the second list as the y-axis values\n",
    "\n",
    "plt.xlabel(\"Tree Length\")\n",
    "plt.ylabel(\"Latency (ms)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "plt.savefig(\"tree_vs_sequential.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f0273",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39c2604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
